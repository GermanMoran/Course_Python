{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Strategies to prevent overfitting in neural networks</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalMaxPooling1D, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "try:\n",
    "    tf.set_random_seed(1337)                    # set the random seed for reproducibility\n",
    "except:\n",
    "    tf.random.set_seed(1337)                     # NOTE: Newer version of tensorflow uses tf.random.set_seed\n",
    "np.random.seed(1337)                         #       instead of tf.set_random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction</h2>\n",
    "<p><strong>Business Context.</strong> You are a data scientist working for a machine learning consultancy. One of your clients wants to be able to classify text reviews automatically by the likely rating (on a 1 - 5 scale) that that person would give. However, they do not have sufficient data they generated on their own to do this, so you need to use an external, rich dataset as a basis on which to build your model and then translate it over.</p>\n",
    "<p><strong>Business Problem.</strong> Your task is to <strong>build a neural networks-based model for classifying text reviews into likely ratings (on a 1 - 5 scale)</strong>.</p>\n",
    "<p><strong>Analytical Context.</strong> We'll use the Amazon review dataset again and try to classify reviews into star ratings automatically. Instead of just positive and negative, we'll take on the harder challenge of predicting the <em>exact</em> star rating. The lowest score is 1 and the highest is 5.</p>\n",
    "<p>Instead of trying to optimize by pre-processing the text, we'll do very basic tokenization and experiment with different neural network models, architectures, and hyperparameters to optimize the results. You'll start by building a simple dense neural network and try to get it to perform better using various techniques. Then you'll evaluate the results and diagnose where it tends to perform more poorly.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Setting up and preparing the data</h2>\n",
    "<p>We'll mainly be using the <code>keras</code> module from TensorFlow, but we'll also use <code>pandas</code> to read the CSV file and <code>sklearn</code> for some helper functions. We'll be using only the \"Text\" and \"Score\" columns in the <code>Reviews.csv</code> file:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_reviews = pd.read_csv('Reviews.csv', nrows=262084)\n",
    "amazon_reviews.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 1:</h3>\n",
    "<p>Combine the first 1,000 of each of the 1-, 2-, 3-, 4-, and 5-star reviews in <code>amazon_reviews</code> into a single DataFrame (so you should have 5,000 observations in total). Split this DataFrame into training and test sets, with 80% of the data for the training set.</p>\n",
    "<p><strong>Hint:</strong> <code>keras</code> will expected your labels to start with 0, and not 1, so make sure to adjust the labels accordingly.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262079</th>\n",
       "      <td>1</td>\n",
       "      <td>Bought Lucy's cookies after I heard about them...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262080</th>\n",
       "      <td>3</td>\n",
       "      <td>I am not sure what I expected but I found the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262081</th>\n",
       "      <td>1</td>\n",
       "      <td>SO disappointed off taste all the flavors tast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262082</th>\n",
       "      <td>2</td>\n",
       "      <td>If you have to eat gluten free, these cookies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262083</th>\n",
       "      <td>1</td>\n",
       "      <td>Lucy's cookies has a track record of buying co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262084 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Score                                               Text\n",
       "0           5  I have bought several of the Vitality canned d...\n",
       "1           1  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2           4  This is a confection that has been around a fe...\n",
       "3           2  If you are looking for the secret ingredient i...\n",
       "4           5  Great taffy at a great price.  There was a wid...\n",
       "...       ...                                                ...\n",
       "262079      1  Bought Lucy's cookies after I heard about them...\n",
       "262080      3  I am not sure what I expected but I found the ...\n",
       "262081      1  SO disappointed off taste all the flavors tast...\n",
       "262082      2  If you have to eat gluten free, these cookies ...\n",
       "262083      1  Lucy's cookies has a track record of buying co...\n",
       "\n",
       "[262084 rows x 2 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we filter the data-set with only text and score\n",
    "amazon_review = amazon_reviews[[\"Score\",\"Text\"]]\n",
    "amazon_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>My cats have been happily eating Felidae Plati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The candy is just red , No flavor . Just  plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>This oatmeal is not good. Its mushy, soft, I d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Arrived in 6 days and were so stale i could no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4</td>\n",
       "      <td>i followed the easy instructions and these tur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4</td>\n",
       "      <td>WE LOVE LOVE LOVE these muffins. They are mois...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4</td>\n",
       "      <td>I brought 2 bottles.  One I carry in my pocket...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4</td>\n",
       "      <td>I really like this soup.  It is mild and I don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>4</td>\n",
       "      <td>This is quick, easy and portable, and I think ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Score                                               Text\n",
       "0         0  Product arrived labeled as Jumbo Salted Peanut...\n",
       "1         0  My cats have been happily eating Felidae Plati...\n",
       "2         0  The candy is just red , No flavor . Just  plan...\n",
       "3         0  This oatmeal is not good. Its mushy, soft, I d...\n",
       "4         0  Arrived in 6 days and were so stale i could no...\n",
       "...     ...                                                ...\n",
       "4995      4  i followed the easy instructions and these tur...\n",
       "4996      4  WE LOVE LOVE LOVE these muffins. They are mois...\n",
       "4997      4  I brought 2 bottles.  One I carry in my pocket...\n",
       "4998      4  I really like this soup.  It is mild and I don...\n",
       "4999      4  This is quick, easy and portable, and I think ...\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we create the dataframe \n",
    "score_one  = amazon_review[amazon_review[\"Score\"]==1][0:1000]\n",
    "score_one[\"Score\"]=0\n",
    "score_two  = amazon_review[amazon_review[\"Score\"]==2][0:1000]\n",
    "score_two[\"Score\"]=1\n",
    "score_three  = amazon_review[amazon_review[\"Score\"]==3][0:1000]\n",
    "score_three[\"Score\"]=2\n",
    "score_four  = amazon_review[amazon_review[\"Score\"]==4][0:1000]\n",
    "score_four[\"Score\"]=3\n",
    "score_five  = amazon_review[amazon_review[\"Score\"]==5][0:1000]\n",
    "score_five[\"Score\"]=4\n",
    "\n",
    "\n",
    "scores = pd.concat([score_one,score_two,score_three,score_four,score_five],ignore_index= True)\n",
    "\n",
    "# Now  I divided the dataset in two groups: Train and Test    random_state = 0\n",
    "train, test = train_test_split(scores, test_size = 0.2, random_state = np.random.seed(1337))\n",
    "\n",
    "scores\n",
    "#x_train, y_train , x_test, y_test = train_test_split(scores, test_size = 0.2, random_state = np.random.seed(1337))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is another form\n",
    "#x_train, x_test, y_train, y_test = train_test_split(scores.Text,scores.Score, test_size=0.2, random_state=np.random.seed(1337))\n",
    "#print(\"number of test samples :\", x_test.shape[0])\n",
    "#print(\"number of training samples:\",x_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Tokenizing our texts</h2>\n",
    "<p>Keras comes with its own functions to preprocess text, including a <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\">tokenizer</a> (a mapping from each word in our corpus to a unique integer). Unlike the <code>CountVectorizer</code> from <code>sklearn</code>, which produces sparse matrices, <code>keras</code> often expects to work with sequences representing only the words that occur in a text. To prepare text before feeding it into a neural network, we usually:</p>\n",
    "<ol>\n",
    "<li>Create a <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\">tokenizer</a>.</li>\n",
    "<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences\">Create sequences</a> from our text (each text becomes a list of integers, based on the tokenizer mapping, instead of words)</li>\n",
    "<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\">Pad or truncate</a> each sequence to a fixed length (very short texts get <code>0</code>s added to them, while very long ones are truncated).</li>\n",
    "</ol>\n",
    "<p>The tokenizer has a configurable word cap, so it will only consider the $n$ most common words in the corpus, ignoring very rare words.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 2:</h3>\n",
    "<p>In this exercise, you will learn how to use the <code>tf.keras.preprocessing.text.Tokenizer</code> tool to carry out the preprocessing steps described above.</p>\n",
    "<h4>2.1</h4>\n",
    "<p>Perform some exploratory analysis of the dataset to calculate the number of unique words in our corpus and the distribution of the number of words in each review of the training set. What is the 80th percentile of this distribution?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116.0"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#First I had random_state=tf.random.set_seed(1337), then I changed it to random_state=np.random.seed(1337) and worked\n",
    "words_per_review_train =train.Text.apply(lambda x: len(x.split()))\n",
    "words_per_review_train.hist(bins = 100)\n",
    "\n",
    "words_per_review_train.quantile(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.2</h4>\n",
    "<p>Given the results above, we create a tokenizer using only the top 20,000 most frequent words in our corpus (which corresponds to roughly 80% of the words): </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000) #We create the tokenizer using only top 20000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train['Text'])  #Then, we create the text->indices mapping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line has given several features and methods to our tokenizer. For instance, print the line `tokenizer.word_index` in a new cell - what do you see? Apply the `tokenizer.texts_to_sequences()` method on the list `['I just feel very very good']`. Apply the `tokenizer.sequences_to_texts()` method on the list `[[109, 19, 824, 76, 114, 6315, 1137, 8070]]`. What were your results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'i': 2,\n",
       " 'a': 3,\n",
       " 'and': 4,\n",
       " 'to': 5,\n",
       " 'it': 6,\n",
       " 'of': 7,\n",
       " 'br': 8,\n",
       " 'is': 9,\n",
       " 'this': 10,\n",
       " 'in': 11,\n",
       " 'for': 12,\n",
       " 'but': 13,\n",
       " 'that': 14,\n",
       " 'not': 15,\n",
       " 'my': 16,\n",
       " 'was': 17,\n",
       " 'with': 18,\n",
       " 'you': 19,\n",
       " 'have': 20,\n",
       " 'are': 21,\n",
       " 'they': 22,\n",
       " 'as': 23,\n",
       " 'like': 24,\n",
       " 'on': 25,\n",
       " 'so': 26,\n",
       " 'these': 27,\n",
       " 'be': 28,\n",
       " 'taste': 29,\n",
       " 'if': 30,\n",
       " 'coffee': 31,\n",
       " 'good': 32,\n",
       " 'product': 33,\n",
       " 'them': 34,\n",
       " 'or': 35,\n",
       " 'just': 36,\n",
       " 'at': 37,\n",
       " 'all': 38,\n",
       " 'very': 39,\n",
       " 'one': 40,\n",
       " 'would': 41,\n",
       " 'flavor': 42,\n",
       " \"it's\": 43,\n",
       " 'can': 44,\n",
       " 'from': 45,\n",
       " 'had': 46,\n",
       " 'more': 47,\n",
       " 'when': 48,\n",
       " 'me': 49,\n",
       " 'will': 50,\n",
       " 'out': 51,\n",
       " 'than': 52,\n",
       " 'has': 53,\n",
       " 'other': 54,\n",
       " 'no': 55,\n",
       " 'great': 56,\n",
       " 'were': 57,\n",
       " 'much': 58,\n",
       " 'we': 59,\n",
       " 'really': 60,\n",
       " 'too': 61,\n",
       " \"don't\": 62,\n",
       " 'only': 63,\n",
       " 'some': 64,\n",
       " 'up': 65,\n",
       " 'there': 66,\n",
       " 'get': 67,\n",
       " 'because': 68,\n",
       " 'about': 69,\n",
       " 'chips': 70,\n",
       " 'an': 71,\n",
       " 'better': 72,\n",
       " 'amazon': 73,\n",
       " 'what': 74,\n",
       " 'tea': 75,\n",
       " 'your': 76,\n",
       " 'sugar': 77,\n",
       " 'which': 78,\n",
       " 'buy': 79,\n",
       " 'little': 80,\n",
       " 'even': 81,\n",
       " 'food': 82,\n",
       " 'also': 83,\n",
       " 'love': 84,\n",
       " 'use': 85,\n",
       " 'do': 86,\n",
       " 'cup': 87,\n",
       " 'time': 88,\n",
       " \"i'm\": 89,\n",
       " 'price': 90,\n",
       " 'drink': 91,\n",
       " 'tried': 92,\n",
       " 'by': 93,\n",
       " 'make': 94,\n",
       " 'after': 95,\n",
       " 'again': 96,\n",
       " 'water': 97,\n",
       " 'try': 98,\n",
       " 'well': 99,\n",
       " 'first': 100,\n",
       " 'am': 101,\n",
       " 'juice': 102,\n",
       " 'any': 103,\n",
       " 'box': 104,\n",
       " 'find': 105,\n",
       " 'been': 106,\n",
       " 'bag': 107,\n",
       " \"i've\": 108,\n",
       " 'did': 109,\n",
       " 'then': 110,\n",
       " '2': 111,\n",
       " 'sweet': 112,\n",
       " 'now': 113,\n",
       " 'best': 114,\n",
       " '3': 115,\n",
       " 'chocolate': 116,\n",
       " 'their': 117,\n",
       " 'made': 118,\n",
       " 'still': 119,\n",
       " 'eat': 120,\n",
       " 'think': 121,\n",
       " 'bought': 122,\n",
       " 'hot': 123,\n",
       " 'used': 124,\n",
       " 'could': 125,\n",
       " 'bit': 126,\n",
       " 'over': 127,\n",
       " 'know': 128,\n",
       " 'found': 129,\n",
       " 'flavors': 130,\n",
       " 'bad': 131,\n",
       " 'something': 132,\n",
       " 'way': 133,\n",
       " '1': 134,\n",
       " 'orange': 135,\n",
       " 'order': 136,\n",
       " 'does': 137,\n",
       " 'want': 138,\n",
       " \"didn't\": 139,\n",
       " 'brand': 140,\n",
       " 'same': 141,\n",
       " 'tastes': 142,\n",
       " 'cups': 143,\n",
       " 'mix': 144,\n",
       " 'free': 145,\n",
       " 'two': 146,\n",
       " 'k': 147,\n",
       " 'he': 148,\n",
       " 'however': 149,\n",
       " 'who': 150,\n",
       " '5': 151,\n",
       " 'she': 152,\n",
       " 'go': 153,\n",
       " 'many': 154,\n",
       " 'say': 155,\n",
       " 'got': 156,\n",
       " 'since': 157,\n",
       " 'thought': 158,\n",
       " 'less': 159,\n",
       " 'ingredients': 160,\n",
       " 'how': 161,\n",
       " 'our': 162,\n",
       " 'soda': 163,\n",
       " 'organic': 164,\n",
       " \"doesn't\": 165,\n",
       " 'most': 166,\n",
       " 'before': 167,\n",
       " 'though': 168,\n",
       " '4': 169,\n",
       " 'nice': 170,\n",
       " 'give': 171,\n",
       " 'salt': 172,\n",
       " 'store': 173,\n",
       " 'pack': 174,\n",
       " 'bags': 175,\n",
       " 'natural': 176,\n",
       " 'different': 177,\n",
       " 'sure': 178,\n",
       " 'enough': 179,\n",
       " \"can't\": 180,\n",
       " 'recommend': 181,\n",
       " 'ordered': 182,\n",
       " 'never': 183,\n",
       " 'into': 184,\n",
       " 'small': 185,\n",
       " 'its': 186,\n",
       " 'products': 187,\n",
       " 'few': 188,\n",
       " 'regular': 189,\n",
       " 'tasted': 190,\n",
       " 'hard': 191,\n",
       " 'off': 192,\n",
       " 'green': 193,\n",
       " 'pretty': 194,\n",
       " 'quality': 195,\n",
       " 'strong': 196,\n",
       " 'may': 197,\n",
       " 'lot': 198,\n",
       " 'size': 199,\n",
       " 'right': 200,\n",
       " 'while': 201,\n",
       " 'her': 202,\n",
       " 'high': 203,\n",
       " 'being': 204,\n",
       " 'going': 205,\n",
       " 'shipping': 206,\n",
       " 'switch': 207,\n",
       " 'fruit': 208,\n",
       " 'back': 209,\n",
       " 'without': 210,\n",
       " 'looking': 211,\n",
       " \"i'd\": 212,\n",
       " 'makes': 213,\n",
       " 'should': 214,\n",
       " 'those': 215,\n",
       " 'favorite': 216,\n",
       " 'see': 217,\n",
       " 'using': 218,\n",
       " 'probably': 219,\n",
       " 'added': 220,\n",
       " 'dog': 221,\n",
       " 'stuff': 222,\n",
       " 'old': 223,\n",
       " 'people': 224,\n",
       " 'down': 225,\n",
       " 'milk': 226,\n",
       " 'per': 227,\n",
       " 'calories': 228,\n",
       " 'another': 229,\n",
       " 'eating': 230,\n",
       " 'every': 231,\n",
       " 'big': 232,\n",
       " 'put': 233,\n",
       " 'disappointed': 234,\n",
       " 'real': 235,\n",
       " 'ever': 236,\n",
       " 'stars': 237,\n",
       " 'add': 238,\n",
       " 'received': 239,\n",
       " 'tasty': 240,\n",
       " 'easy': 241,\n",
       " '6': 242,\n",
       " 'each': 243,\n",
       " 'away': 244,\n",
       " 'whole': 245,\n",
       " 'thing': 246,\n",
       " 'day': 247,\n",
       " 'quite': 248,\n",
       " 'need': 249,\n",
       " 'fresh': 250,\n",
       " 'money': 251,\n",
       " 'purchase': 252,\n",
       " 'both': 253,\n",
       " '100': 254,\n",
       " 'com': 255,\n",
       " 'here': 256,\n",
       " \"that's\": 257,\n",
       " 'definitely': 258,\n",
       " 'buying': 259,\n",
       " 'problem': 260,\n",
       " 'said': 261,\n",
       " 'work': 262,\n",
       " 'actually': 263,\n",
       " 'maybe': 264,\n",
       " 'years': 265,\n",
       " 'cookies': 266,\n",
       " 'almost': 267,\n",
       " 'anything': 268,\n",
       " 'package': 269,\n",
       " 'gluten': 270,\n",
       " 'always': 271,\n",
       " 'potato': 272,\n",
       " 'reviews': 273,\n",
       " 'full': 274,\n",
       " 'why': 275,\n",
       " 'enjoy': 276,\n",
       " 'tangerine': 277,\n",
       " 'far': 278,\n",
       " 'local': 279,\n",
       " 'amount': 280,\n",
       " \"you're\": 281,\n",
       " \"won't\": 282,\n",
       " 'delicious': 283,\n",
       " 'case': 284,\n",
       " 'long': 285,\n",
       " 'healthy': 286,\n",
       " 'purchased': 287,\n",
       " 'half': 288,\n",
       " 'flavored': 289,\n",
       " 'kettle': 290,\n",
       " 'tasting': 291,\n",
       " 'oz': 292,\n",
       " \"i'll\": 293,\n",
       " '8': 294,\n",
       " 'might': 295,\n",
       " 'worth': 296,\n",
       " 'keurig': 297,\n",
       " 'through': 298,\n",
       " 'blend': 299,\n",
       " 'roast': 300,\n",
       " \"isn't\": 301,\n",
       " 'instead': 302,\n",
       " 'own': 303,\n",
       " 'item': 304,\n",
       " 'corn': 305,\n",
       " 'expensive': 306,\n",
       " 'kind': 307,\n",
       " 'texture': 308,\n",
       " 'new': 309,\n",
       " 'brands': 310,\n",
       " 'keep': 311,\n",
       " 'cocoa': 312,\n",
       " 'fat': 313,\n",
       " 'ok': 314,\n",
       " 'carbonated': 315,\n",
       " 'take': 316,\n",
       " 'nothing': 317,\n",
       " 'several': 318,\n",
       " 'black': 319,\n",
       " 'company': 320,\n",
       " 'low': 321,\n",
       " 'drinking': 322,\n",
       " 'dark': 323,\n",
       " 'oil': 324,\n",
       " 'came': 325,\n",
       " 'cat': 326,\n",
       " 'foods': 327,\n",
       " 'diet': 328,\n",
       " 'either': 329,\n",
       " 'coconut': 330,\n",
       " '12': 331,\n",
       " 'fine': 332,\n",
       " 'perfect': 333,\n",
       " 'rather': 334,\n",
       " 'three': 335,\n",
       " 'usually': 336,\n",
       " 'www': 337,\n",
       " 'review': 338,\n",
       " 'around': 339,\n",
       " 'wanted': 340,\n",
       " 'having': 341,\n",
       " 'http': 342,\n",
       " 'light': 343,\n",
       " 'snack': 344,\n",
       " 'trying': 345,\n",
       " 'ones': 346,\n",
       " 'such': 347,\n",
       " 'packaging': 348,\n",
       " 'sauce': 349,\n",
       " 'last': 350,\n",
       " 'feel': 351,\n",
       " 'gp': 352,\n",
       " 'gave': 353,\n",
       " 'white': 354,\n",
       " 'rice': 355,\n",
       " 'baby': 356,\n",
       " 'aftertaste': 357,\n",
       " 'bitter': 358,\n",
       " 'cans': 359,\n",
       " 'href': 360,\n",
       " 'extra': 361,\n",
       " 'us': 362,\n",
       " 'where': 363,\n",
       " 'liked': 364,\n",
       " 'grocery': 365,\n",
       " 'things': 366,\n",
       " 'open': 367,\n",
       " 'variety': 368,\n",
       " 'ounce': 369,\n",
       " 'his': 370,\n",
       " 'getting': 371,\n",
       " 'smell': 372,\n",
       " 'seems': 373,\n",
       " 'come': 374,\n",
       " 'bold': 375,\n",
       " 'mountain': 376,\n",
       " 'syrup': 377,\n",
       " \"they're\": 378,\n",
       " \"wasn't\": 379,\n",
       " 'drinks': 380,\n",
       " 'weak': 381,\n",
       " 'vanilla': 382,\n",
       " 'least': 383,\n",
       " 'year': 384,\n",
       " 'artificial': 385,\n",
       " 'arrived': 386,\n",
       " 'cake': 387,\n",
       " 'top': 388,\n",
       " 'him': 389,\n",
       " 'others': 390,\n",
       " 'powder': 391,\n",
       " 'chicken': 392,\n",
       " 'making': 393,\n",
       " 'although': 394,\n",
       " 'once': 395,\n",
       " 'read': 396,\n",
       " \"couldn't\": 397,\n",
       " 'side': 398,\n",
       " 'decaf': 399,\n",
       " '10': 400,\n",
       " 'loves': 401,\n",
       " 'comes': 402,\n",
       " 'dogs': 403,\n",
       " 'salty': 404,\n",
       " 'boxes': 405,\n",
       " 'chip': 406,\n",
       " 'excellent': 407,\n",
       " 'dry': 408,\n",
       " 'fact': 409,\n",
       " 'seem': 410,\n",
       " 'decided': 411,\n",
       " 'beans': 412,\n",
       " 'cats': 413,\n",
       " 'cost': 414,\n",
       " 'expected': 415,\n",
       " 'else': 416,\n",
       " 'prefer': 417,\n",
       " 'look': 418,\n",
       " \"wouldn't\": 419,\n",
       " 'times': 420,\n",
       " 'until': 421,\n",
       " 'save': 422,\n",
       " 'cheaper': 423,\n",
       " 'husband': 424,\n",
       " 'mouth': 425,\n",
       " 'happy': 426,\n",
       " 'stick': 427,\n",
       " 'stores': 428,\n",
       " 'next': 429,\n",
       " 'cream': 430,\n",
       " 'wish': 431,\n",
       " 'alternative': 432,\n",
       " 'ingredient': 433,\n",
       " 'ground': 434,\n",
       " 'large': 435,\n",
       " 'apple': 436,\n",
       " 'months': 437,\n",
       " 'guess': 438,\n",
       " 'opened': 439,\n",
       " 'believe': 440,\n",
       " 'fan': 441,\n",
       " 'candy': 442,\n",
       " 'wonderful': 443,\n",
       " 'deal': 444,\n",
       " 'treat': 445,\n",
       " 'available': 446,\n",
       " 'home': 447,\n",
       " 'says': 448,\n",
       " 'especially': 449,\n",
       " 'vinegar': 450,\n",
       " 'bottom': 451,\n",
       " 'medium': 452,\n",
       " 'family': 453,\n",
       " 'kids': 454,\n",
       " 'couple': 455,\n",
       " 'health': 456,\n",
       " 'took': 457,\n",
       " 'bottle': 458,\n",
       " 'days': 459,\n",
       " 'soup': 460,\n",
       " 'idea': 461,\n",
       " 'second': 462,\n",
       " 'spicy': 463,\n",
       " 'plus': 464,\n",
       " 'wolfgang': 465,\n",
       " 'plastic': 466,\n",
       " 'left': 467,\n",
       " 'treats': 468,\n",
       " 'yet': 469,\n",
       " 'overall': 470,\n",
       " 'unfortunately': 471,\n",
       " 'loved': 472,\n",
       " 'carbonation': 473,\n",
       " 'serving': 474,\n",
       " 'coffees': 475,\n",
       " 'vitamin': 476,\n",
       " 'cheap': 477,\n",
       " 'expect': 478,\n",
       " 'pieces': 479,\n",
       " 'ordering': 480,\n",
       " '24': 481,\n",
       " 'reason': 482,\n",
       " 'energy': 483,\n",
       " 'french': 484,\n",
       " 'let': 485,\n",
       " 'brown': 486,\n",
       " 'went': 487,\n",
       " 'able': 488,\n",
       " 'slightly': 489,\n",
       " 'cookie': 490,\n",
       " 'started': 491,\n",
       " 'morning': 492,\n",
       " 'type': 493,\n",
       " 'breakfast': 494,\n",
       " 'broken': 495,\n",
       " 'wrong': 496,\n",
       " 'label': 497,\n",
       " 'contains': 498,\n",
       " \"there's\": 499,\n",
       " 'quick': 500,\n",
       " 'machine': 501,\n",
       " '50': 502,\n",
       " \"you'll\": 503,\n",
       " 'minutes': 504,\n",
       " 'anyone': 505,\n",
       " 'cheese': 506,\n",
       " 'end': 507,\n",
       " 'mixed': 508,\n",
       " 'puck': 509,\n",
       " 'someone': 510,\n",
       " 'beverage': 511,\n",
       " 'brew': 512,\n",
       " 'pay': 513,\n",
       " 'enjoyed': 514,\n",
       " 'meat': 515,\n",
       " 'experience': 516,\n",
       " 'soy': 517,\n",
       " 'protein': 518,\n",
       " 'pancakes': 519,\n",
       " 'son': 520,\n",
       " 'c': 521,\n",
       " 'certainly': 522,\n",
       " 'formula': 523,\n",
       " 'name': 524,\n",
       " 'similar': 525,\n",
       " 'market': 526,\n",
       " 'waste': 527,\n",
       " 'list': 528,\n",
       " 'care': 529,\n",
       " 'plain': 530,\n",
       " 'value': 531,\n",
       " 'packaged': 532,\n",
       " 'butter': 533,\n",
       " 'line': 534,\n",
       " 'smooth': 535,\n",
       " 'single': 536,\n",
       " 'sour': 537,\n",
       " 'flour': 538,\n",
       " 'help': 539,\n",
       " 'aroma': 540,\n",
       " '20': 541,\n",
       " 'absolutely': 542,\n",
       " 'online': 543,\n",
       " 'etc': 544,\n",
       " 'everything': 545,\n",
       " 'month': 546,\n",
       " 'healthier': 547,\n",
       " 'honey': 548,\n",
       " 'unless': 549,\n",
       " 'choice': 550,\n",
       " 'okay': 551,\n",
       " 'starbucks': 552,\n",
       " 'microwave': 553,\n",
       " 'longer': 554,\n",
       " 'ago': 555,\n",
       " 'myself': 556,\n",
       " 'opinion': 557,\n",
       " 'highly': 558,\n",
       " 'baking': 559,\n",
       " 'based': 560,\n",
       " 'lemon': 561,\n",
       " '9': 562,\n",
       " \"haven't\": 563,\n",
       " 'iced': 564,\n",
       " 'likes': 565,\n",
       " 'meal': 566,\n",
       " 'soft': 567,\n",
       " 'grape': 568,\n",
       " 'non': 569,\n",
       " 'part': 570,\n",
       " 'description': 571,\n",
       " 'friends': 572,\n",
       " 'hoping': 573,\n",
       " 'yes': 574,\n",
       " 'blue': 575,\n",
       " \"earth's\": 576,\n",
       " 'pods': 577,\n",
       " 'must': 578,\n",
       " 'please': 579,\n",
       " 'cannot': 580,\n",
       " 'otherwise': 581,\n",
       " 'looked': 582,\n",
       " 'works': 583,\n",
       " 'ice': 584,\n",
       " 'decent': 585,\n",
       " 'gift': 586,\n",
       " 'inside': 587,\n",
       " 'stale': 588,\n",
       " 'house': 589,\n",
       " 'caramel': 590,\n",
       " 'extremely': 591,\n",
       " '7': 592,\n",
       " 'star': 593,\n",
       " 'awful': 594,\n",
       " 'cereal': 595,\n",
       " 'terrible': 596,\n",
       " 'packs': 597,\n",
       " 'cut': 598,\n",
       " 'crackers': 599,\n",
       " 'ounces': 600,\n",
       " 'itself': 601,\n",
       " 'rich': 602,\n",
       " 'leaves': 603,\n",
       " 'pork': 604,\n",
       " 'given': 605,\n",
       " 'version': 606,\n",
       " 'recipe': 607,\n",
       " 'rest': 608,\n",
       " 'popcorn': 609,\n",
       " 'needed': 610,\n",
       " 'compared': 611,\n",
       " 'mild': 612,\n",
       " 'mind': 613,\n",
       " 'super': 614,\n",
       " 'already': 615,\n",
       " 'cherry': 616,\n",
       " 'horrible': 617,\n",
       " \"aren't\": 618,\n",
       " 'ginger': 619,\n",
       " 'sodium': 620,\n",
       " 'calorie': 621,\n",
       " 'contain': 622,\n",
       " 'items': 623,\n",
       " 'due': 624,\n",
       " 'weeks': 625,\n",
       " 'during': 626,\n",
       " 'teas': 627,\n",
       " 'canned': 628,\n",
       " 'often': 629,\n",
       " 'stevia': 630,\n",
       " 'smaller': 631,\n",
       " 'sometimes': 632,\n",
       " 'nuts': 633,\n",
       " 'perhaps': 634,\n",
       " 'week': 635,\n",
       " 'return': 636,\n",
       " 'giving': 637,\n",
       " 'none': 638,\n",
       " 'easily': 639,\n",
       " 'pop': 640,\n",
       " 'called': 641,\n",
       " 'pepper': 642,\n",
       " 'juices': 643,\n",
       " 'problems': 644,\n",
       " 'point': 645,\n",
       " 'tell': 646,\n",
       " 'everyone': 647,\n",
       " 'date': 648,\n",
       " 'color': 649,\n",
       " 'surprised': 650,\n",
       " 'finally': 651,\n",
       " 'shipped': 652,\n",
       " 'packages': 653,\n",
       " 'recommended': 654,\n",
       " 'results': 655,\n",
       " 'service': 656,\n",
       " 'past': 657,\n",
       " 'larger': 658,\n",
       " 'difference': 659,\n",
       " 'expecting': 660,\n",
       " 'higher': 661,\n",
       " 'peanut': 662,\n",
       " 'somewhat': 663,\n",
       " 'jars': 664,\n",
       " 'acid': 665,\n",
       " 'wife': 666,\n",
       " 'looks': 667,\n",
       " 'change': 668,\n",
       " 'thinking': 669,\n",
       " 'flavorful': 670,\n",
       " 'red': 671,\n",
       " 'note': 672,\n",
       " 'bland': 673,\n",
       " 'exactly': 674,\n",
       " 'weight': 675,\n",
       " 'seemed': 676,\n",
       " 'e': 677,\n",
       " 'together': 678,\n",
       " 'worst': 679,\n",
       " 'hand': 680,\n",
       " '15': 681,\n",
       " 'life': 682,\n",
       " 'original': 683,\n",
       " 'bars': 684,\n",
       " 'hope': 685,\n",
       " 'issue': 686,\n",
       " 'container': 687,\n",
       " '16': 688,\n",
       " 'wheat': 689,\n",
       " 'customer': 690,\n",
       " 'saw': 691,\n",
       " 'true': 692,\n",
       " 'disappointing': 693,\n",
       " 'done': 694,\n",
       " 'mess': 695,\n",
       " 'recently': 696,\n",
       " 'grams': 697,\n",
       " 'close': 698,\n",
       " 'sold': 699,\n",
       " 'sweetener': 700,\n",
       " 'difficult': 701,\n",
       " 'sale': 702,\n",
       " 'paid': 703,\n",
       " 'bite': 704,\n",
       " 'four': 705,\n",
       " 'seeds': 706,\n",
       " 'batch': 707,\n",
       " 'purchasing': 708,\n",
       " 'place': 709,\n",
       " 'website': 710,\n",
       " 'count': 711,\n",
       " 'glass': 712,\n",
       " 'except': 713,\n",
       " 'five': 714,\n",
       " 'special': 715,\n",
       " 'pound': 716,\n",
       " 'adding': 717,\n",
       " 'excited': 718,\n",
       " 'refreshing': 719,\n",
       " 'clean': 720,\n",
       " 'pineapple': 721,\n",
       " 'licorice': 722,\n",
       " 'crunchy': 723,\n",
       " 'bar': 724,\n",
       " 'jar': 725,\n",
       " 'potatoes': 726,\n",
       " 'cook': 727,\n",
       " 'changed': 728,\n",
       " 'option': 729,\n",
       " '0': 730,\n",
       " 'dried': 731,\n",
       " 'course': 732,\n",
       " 'throw': 733,\n",
       " 'nasty': 734,\n",
       " 'sent': 735,\n",
       " 'noticed': 736,\n",
       " 'anyway': 737,\n",
       " 'daughter': 738,\n",
       " 'normal': 739,\n",
       " 'thanks': 740,\n",
       " 'fast': 741,\n",
       " 'twice': 742,\n",
       " 'baked': 743,\n",
       " 'content': 744,\n",
       " 'o': 745,\n",
       " 'simply': 746,\n",
       " 'watery': 747,\n",
       " 'short': 748,\n",
       " 'normally': 749,\n",
       " 'felt': 750,\n",
       " 'stuck': 751,\n",
       " 'takes': 752,\n",
       " 'needs': 753,\n",
       " 'shop': 754,\n",
       " 'instant': 755,\n",
       " 'thick': 756,\n",
       " 'sweetness': 757,\n",
       " 'rinds': 758,\n",
       " 'grounds': 759,\n",
       " 'fair': 760,\n",
       " 'caffeine': 761,\n",
       " 'packets': 762,\n",
       " 'doing': 763,\n",
       " 'oatmeal': 764,\n",
       " 'thin': 765,\n",
       " 'glad': 766,\n",
       " 'avoid': 767,\n",
       " 'strawberry': 768,\n",
       " '00': 769,\n",
       " 'fairly': 770,\n",
       " 'bottles': 771,\n",
       " 'listed': 772,\n",
       " 'consistency': 773,\n",
       " 'yourself': 774,\n",
       " 'huge': 775,\n",
       " 'thank': 776,\n",
       " 'tiny': 777,\n",
       " 'sodas': 778,\n",
       " 'told': 779,\n",
       " 'between': 780,\n",
       " 'mostly': 781,\n",
       " 'goes': 782,\n",
       " 'spice': 783,\n",
       " 'melitta': 784,\n",
       " 'level': 785,\n",
       " 'sugars': 786,\n",
       " 'entire': 787,\n",
       " 'ate': 788,\n",
       " 'particular': 789,\n",
       " 'sort': 790,\n",
       " 'brewing': 791,\n",
       " 'raw': 792,\n",
       " 'subscribe': 793,\n",
       " 'world': 794,\n",
       " 'beef': 795,\n",
       " 'dinner': 796,\n",
       " 'amazing': 797,\n",
       " 'gum': 798,\n",
       " 'bean': 799,\n",
       " 'berry': 800,\n",
       " 'cold': 801,\n",
       " 'heat': 802,\n",
       " 'liquid': 803,\n",
       " 'ended': 804,\n",
       " 'cause': 805,\n",
       " 'feeding': 806,\n",
       " 'feed': 807,\n",
       " 'shipment': 808,\n",
       " 'today': 809,\n",
       " 'clear': 810,\n",
       " 'lots': 811,\n",
       " 'consider': 812,\n",
       " 'finish': 813,\n",
       " 'noodles': 814,\n",
       " 'gives': 815,\n",
       " 'start': 816,\n",
       " 'completely': 817,\n",
       " 'biscuits': 818,\n",
       " 'along': 819,\n",
       " 'run': 820,\n",
       " '30': 821,\n",
       " 'later': 822,\n",
       " 'stay': 823,\n",
       " 'miss': 824,\n",
       " 'balance': 825,\n",
       " 'fish': 826,\n",
       " 'check': 827,\n",
       " 'kiwi': 828,\n",
       " 'fiber': 829,\n",
       " 'body': 830,\n",
       " 'sea': 831,\n",
       " 'nearly': 832,\n",
       " 'reading': 833,\n",
       " 'stop': 834,\n",
       " 'flavoring': 835,\n",
       " 'weird': 836,\n",
       " 'gets': 837,\n",
       " 'sealed': 838,\n",
       " 'yummy': 839,\n",
       " 'cooked': 840,\n",
       " 'stock': 841,\n",
       " 'basically': 842,\n",
       " 'cooking': 843,\n",
       " 'six': 844,\n",
       " 'means': 845,\n",
       " 'sticks': 846,\n",
       " 'reviewer': 847,\n",
       " 'set': 848,\n",
       " 'pod': 849,\n",
       " 'fit': 850,\n",
       " 'filter': 851,\n",
       " 'paying': 852,\n",
       " 'air': 853,\n",
       " 'soon': 854,\n",
       " 'bisquick': 855,\n",
       " 'mixes': 856,\n",
       " 'number': 857,\n",
       " 'quickly': 858,\n",
       " 'setting': 859,\n",
       " 'easier': 860,\n",
       " 'sorry': 861,\n",
       " 'flat': 862,\n",
       " 'future': 863,\n",
       " 'china': 864,\n",
       " 'within': 865,\n",
       " 'considering': 866,\n",
       " 'cinnamon': 867,\n",
       " 'under': 868,\n",
       " 'tomatoes': 869,\n",
       " 'filling': 870,\n",
       " 'leave': 871,\n",
       " 'live': 872,\n",
       " 'drinker': 873,\n",
       " 'whatever': 874,\n",
       " 'cents': 875,\n",
       " 'call': 876,\n",
       " 'form': 877,\n",
       " 'daily': 878,\n",
       " 'mine': 879,\n",
       " 'including': 880,\n",
       " 'bulk': 881,\n",
       " 'sparkling': 882,\n",
       " 'pasta': 883,\n",
       " 'jerky': 884,\n",
       " '25': 885,\n",
       " 'lid': 886,\n",
       " 'pure': 887,\n",
       " 'agree': 888,\n",
       " 'remember': 889,\n",
       " 'hate': 890,\n",
       " 'gourmet': 891,\n",
       " 'mean': 892,\n",
       " 'eaten': 893,\n",
       " 'picture': 894,\n",
       " 'smells': 895,\n",
       " 'oh': 896,\n",
       " 'supposed': 897,\n",
       " 'hours': 898,\n",
       " 'suggest': 899,\n",
       " 'packed': 900,\n",
       " 'worse': 901,\n",
       " 'combination': 902,\n",
       " 'anymore': 903,\n",
       " 'lunch': 904,\n",
       " 'pleasant': 905,\n",
       " 'totally': 906,\n",
       " 'egg': 907,\n",
       " 'crunch': 908,\n",
       " 'preservatives': 909,\n",
       " 'beverages': 910,\n",
       " 'gone': 911,\n",
       " 'chew': 912,\n",
       " 'dont': 913,\n",
       " 'certified': 914,\n",
       " 'mint': 915,\n",
       " 'overly': 916,\n",
       " 'filled': 917,\n",
       " 'crisp': 918,\n",
       " 'varieties': 919,\n",
       " 'serve': 920,\n",
       " 'directions': 921,\n",
       " 'maker': 922,\n",
       " 'refund': 923,\n",
       " 'christmas': 924,\n",
       " 'instructions': 925,\n",
       " 'upon': 926,\n",
       " 'delivery': 927,\n",
       " 'mistake': 928,\n",
       " 'teeth': 929,\n",
       " 'seller': 930,\n",
       " 'actual': 931,\n",
       " 'issues': 932,\n",
       " 'unlike': 933,\n",
       " 'extract': 934,\n",
       " 'middle': 935,\n",
       " 'thai': 936,\n",
       " 'fructose': 937,\n",
       " 'sucralose': 938,\n",
       " 'brewed': 939,\n",
       " 'disappointment': 940,\n",
       " 'process': 941,\n",
       " 'garlic': 942,\n",
       " 'notice': 943,\n",
       " 'peppermint': 944,\n",
       " 'usual': 945,\n",
       " 'average': 946,\n",
       " 'children': 947,\n",
       " 'break': 948,\n",
       " 'snacks': 949,\n",
       " 'bread': 950,\n",
       " 'cane': 951,\n",
       " 'picky': 952,\n",
       " 'chewy': 953,\n",
       " 'pleased': 954,\n",
       " 'heavy': 955,\n",
       " 'hint': 956,\n",
       " 'fantastic': 957,\n",
       " 'carbs': 958,\n",
       " '40': 959,\n",
       " 'chemical': 960,\n",
       " 'sell': 961,\n",
       " 'barely': 962,\n",
       " 'hit': 963,\n",
       " 'tangy': 964,\n",
       " 'packet': 965,\n",
       " 'immediately': 966,\n",
       " 'seen': 967,\n",
       " 'bowl': 968,\n",
       " 'convenience': 969,\n",
       " 'stopped': 970,\n",
       " 'msg': 971,\n",
       " 'plant': 972,\n",
       " '11': 973,\n",
       " 'likely': 974,\n",
       " 'roasted': 975,\n",
       " 'odd': 976,\n",
       " 'pick': 977,\n",
       " 'delivered': 978,\n",
       " 'convenient': 979,\n",
       " 'send': 980,\n",
       " 'offered': 981,\n",
       " 'friend': 982,\n",
       " 'fun': 983,\n",
       " 'matter': 984,\n",
       " 'condition': 985,\n",
       " 'paper': 986,\n",
       " 'slight': 987,\n",
       " 'minute': 988,\n",
       " 'lower': 989,\n",
       " 'claims': 990,\n",
       " 'eats': 991,\n",
       " 'greasy': 992,\n",
       " 'pancake': 993,\n",
       " 'brewer': 994,\n",
       " 'complaint': 995,\n",
       " 'carry': 996,\n",
       " 'helps': 997,\n",
       " 'calcium': 998,\n",
       " 'nutrition': 999,\n",
       " 'simple': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 36, 351, 39, 39, 32]]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['I just feel very very good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['did you miss your best data science professor']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[109, 19, 824, 76, 114, 6315, 1137, 8070]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.3</h4>\n",
    "<p>Use the tokenizer to transform the texts in our test and train data to sequences. Then, use the <code>pad_sequences</code> function to pad/truncate these sequences to length 116 (the 80th percentile of text lengths). Save the resulting arrays as <code>train_sequences</code> and <code>test_sequences</code>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4614    This gravy mix is excellent ... except, don't ...\n",
       "607     I bought a box, waist'd my $$ but people love ...\n",
       "4219    These creamers come in 4 packs of 25. I have t...\n",
       "3547    I bought the Blue Icicle because I had to quic...\n",
       "3876    This tea makes extremely dark and strong tea o...\n",
       "                              ...                        \n",
       "3239    I loved this product, and it was better than w...\n",
       "1256    I ordered these despite my concerns after read...\n",
       "860     The coffee tasted bitter and like it was burnt...\n",
       "189     you won't BELIEVE how many ways this product h...\n",
       "3223    Possibly THE best salt and vinegar chips (heal...\n",
       "Name: Text, Length: 4000, dtype: object"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the tokenizer to transform the texts in our train data to sequences\n",
    "sequence_train = tokenizer.texts_to_sequences(train.Text)\n",
    "#sequence_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 116)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we obtein the array\n",
    "train_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequence_train,maxlen=116)\n",
    "train_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 116)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# we use the tokenizer to transform the texts in our test data to sequences\n",
    "sequence_test = tokenizer.texts_to_sequences(test.Text)\n",
    "# We obtein the array\n",
    "test_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequence_test,maxlen=116)\n",
    "test_sequences.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Building a basic neural network model</h2>\n",
    "<p>Now that we have preprocessed the text, let's create a basic neural network to train on our data. We'll use an embedding layer which performs <a href=\"https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\">one-hot encoding</a> on our word sequences, two fully connected (\"dense\") layers, and an output layer with 5 neurons to represent the 5 possible star ratings.</p>\n",
    "<p>Before we train a <code>keras</code> model, there is an additional <code>compile</code> step where we define what loss function and optimizer to use, and what metrics to output. Then we can train the model using the <code>fit</code> function. All of this is shown below.</p>\n",
    "<p>Note the <code>validation_split=0.2</code> argument which tells Keras to train on only 80% of the training data and tune the model on the remaining 20%, which we call the validation set. You can see the accuracy and loss for both the training and validation set in the output for each epoch:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=116))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(5, activation='sigmoid'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 116, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 116, 128)          16512     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 116, 128)          16512     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,593,669\n",
      "Trainable params: 2,593,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4614</th>\n",
       "      <td>4</td>\n",
       "      <td>This gravy mix is excellent ... except, don't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>0</td>\n",
       "      <td>I bought a box, waist'd my $$ but people love ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4219</th>\n",
       "      <td>4</td>\n",
       "      <td>These creamers come in 4 packs of 25. I have t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3547</th>\n",
       "      <td>3</td>\n",
       "      <td>I bought the Blue Icicle because I had to quic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>3</td>\n",
       "      <td>This tea makes extremely dark and strong tea o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3239</th>\n",
       "      <td>3</td>\n",
       "      <td>I loved this product, and it was better than w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>1</td>\n",
       "      <td>I ordered these despite my concerns after read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0</td>\n",
       "      <td>The coffee tasted bitter and like it was burnt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0</td>\n",
       "      <td>you won't BELIEVE how many ways this product h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>3</td>\n",
       "      <td>Possibly THE best salt and vinegar chips (heal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Score                                               Text\n",
       "4614      4  This gravy mix is excellent ... except, don't ...\n",
       "607       0  I bought a box, waist'd my $$ but people love ...\n",
       "4219      4  These creamers come in 4 packs of 25. I have t...\n",
       "3547      3  I bought the Blue Icicle because I had to quic...\n",
       "3876      3  This tea makes extremely dark and strong tea o...\n",
       "...     ...                                                ...\n",
       "3239      3  I loved this product, and it was better than w...\n",
       "1256      1  I ordered these despite my concerns after read...\n",
       "860       0  The coffee tasted bitter and like it was burnt...\n",
       "189       0  you won't BELIEVE how many ways this product h...\n",
       "3223      3  Possibly THE best salt and vinegar chips (heal...\n",
       "\n",
       "[4000 rows x 2 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 4, ..., 0, 0, 3], dtype=int64)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create the arrays numpy\n",
    "# This is another form : labels1=train.Score.to_numpy()\n",
    "\n",
    "labels = np.array(np.array(train.Score))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 1.5802 - acc: 0.2747 - val_loss: 1.5186 - val_acc: 0.3587\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 1.3718 - acc: 0.4297 - val_loss: 1.2999 - val_acc: 0.4725\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 1.0587 - acc: 0.5784 - val_loss: 1.2185 - val_acc: 0.5163\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 0.7233 - acc: 0.7441 - val_loss: 1.2806 - val_acc: 0.5050\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 0.4182 - acc: 0.8788 - val_loss: 1.4395 - val_acc: 0.4750\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 0.2158 - acc: 0.9572 - val_loss: 1.6261 - val_acc: 0.4787\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 0.1059 - acc: 0.9822 - val_loss: 1.7737 - val_acc: 0.4650\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 0.0534 - acc: 0.9937 - val_loss: 1.9159 - val_acc: 0.4663\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 0.0312 - acc: 0.9972 - val_loss: 2.0444 - val_acc: 0.4588\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 0.0209 - acc: 0.9981 - val_loss: 2.1236 - val_acc: 0.4638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b0abe81fc8>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sequences, labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 3:</h3>\n",
    "<p>How well does this model perform? How does this compare to a baseline expectation? What do you notice about the accuracy and loss values for both the validation and training sets over time and what does this mean?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that for the training set, the loss values decrease with time, while the accuracy values increase, approaching very close to 1, which means that it is good, however for the validation set it is observed that the Loss values decrease to some extent, but then start to increase. In the same way, it is observed that for the accuracy values, they never improve, there are always almost constants around 0.4. This means that the network does not perform as well when using the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 151us/sample - loss: 2.3488 - acc: 0.4370\n",
      "Accurancy:  0.437\n"
     ]
    }
   ],
   "source": [
    "#X_test_std = scaler.transform(test.Text)\n",
    "import math\n",
    "y_test = test.Score.to_numpy()\n",
    "loss ,acc = model.evaluate(test_sequences, y_test)\n",
    "\n",
    "print(\"Accurancy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy tells us that the model that was applied applied to data that the model had never seen before, predicts 43.7% of them correctly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 2.3841858e-07, 3.2961369e-05, 1.1435151e-04,\n",
       "        4.7683716e-07],\n",
       "       [9.0569258e-04, 3.5610199e-03, 6.4998865e-05, 2.7829409e-04,\n",
       "        1.2171507e-02],\n",
       "       [1.3411045e-06, 1.4901161e-07, 6.1839819e-05, 6.0576200e-04,\n",
       "        3.8743019e-07],\n",
       "       ...,\n",
       "       [0.0000000e+00, 2.2351742e-06, 2.5099516e-03, 9.9539757e-06,\n",
       "        3.0100346e-06],\n",
       "       [8.6426735e-06, 1.3411045e-06, 1.2725592e-05, 1.3759732e-04,\n",
       "        3.9296150e-03],\n",
       "       [2.3841858e-07, 2.9802322e-08, 3.5762787e-07, 2.4383366e-03,\n",
       "        1.3588905e-02]], dtype=float32)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WE predict the output with the test set\n",
    "#a = model.predict_proba(test_sequences)\n",
    "\n",
    "probabilities = model.predict_proba(test_sequences)\n",
    "values = model.predict(test_sequences)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_predict = np.argmax(values , axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = []\n",
    "for x,y in enumerate(values):\n",
    "    index_max = np.argmax(values[x])\n",
    "    lis.append(index_max)\n",
    "    \n",
    "    \n",
    "array_predict = np.array(lis)\n",
    "#array_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 3, 4, 3, 2, 3, 1, 4, 1, 4, 0, 0, 3, 3, 3, 1, 2, 3, 2, 1, 3,\n",
       "       0, 0, 4, 2, 0, 3, 0, 2, 3, 2, 2, 1, 4, 1, 1, 3, 3, 3, 4, 4, 2, 1,\n",
       "       1, 0, 2, 2, 1, 2, 1, 4, 1, 4, 2, 0, 3, 2, 0, 2, 4, 3, 2, 4, 4, 0,\n",
       "       4, 1, 1, 4, 3, 4, 1, 1, 2, 1, 1, 4, 3, 0, 3, 4, 4, 0, 3, 0, 4, 3,\n",
       "       2, 2, 3, 3, 4, 0, 3, 4, 1, 3, 0, 0, 0, 3, 0, 0, 1, 2, 1, 3, 3, 4,\n",
       "       3, 4, 2, 2, 0, 3, 1, 3, 4, 3, 1, 1, 0, 4, 2, 4, 1, 3, 0, 4, 2, 1,\n",
       "       1, 0, 4, 0, 1, 4, 4, 2, 3, 0, 4, 0, 4, 0, 0, 2, 4, 2, 1, 2, 4, 4,\n",
       "       4, 2, 4, 0, 3, 0, 4, 1, 4, 4, 3, 4, 0, 1, 3, 4, 0, 2, 2, 0, 2, 2,\n",
       "       0, 1, 0, 2, 4, 3, 4, 3, 4, 3, 4, 4, 3, 0, 1, 2, 1, 4, 1, 4, 3, 1,\n",
       "       0, 3, 0, 1, 2, 1, 3, 2, 1, 0, 0, 3, 3, 4, 3, 0, 4, 0, 4, 2, 3, 4,\n",
       "       2, 1, 4, 3, 1, 1, 1, 4, 3, 0, 0, 0, 3, 1, 2, 0, 0, 2, 1, 0, 1, 3,\n",
       "       2, 1, 3, 2, 0, 3, 1, 2, 4, 3, 0, 4, 3, 2, 1, 3, 4, 0, 4, 2, 3, 1,\n",
       "       4, 0, 4, 3, 3, 0, 3, 0, 1, 3, 2, 0, 2, 4, 3, 1, 0, 0, 3, 3, 0, 2,\n",
       "       1, 0, 0, 3, 3, 2, 1, 2, 0, 2, 3, 2, 2, 1, 2, 4, 4, 2, 1, 4, 1, 2,\n",
       "       0, 4, 1, 0, 2, 4, 3, 4, 4, 2, 3, 0, 1, 2, 4, 4, 4, 1, 3, 2, 2, 4,\n",
       "       3, 2, 3, 4, 0, 3, 4, 0, 0, 1, 2, 3, 3, 1, 3, 3, 3, 2, 2, 3, 0, 2,\n",
       "       2, 3, 1, 3, 4, 0, 1, 4, 4, 3, 0, 3, 4, 0, 0, 3, 4, 2, 2, 0, 2, 3,\n",
       "       1, 1, 3, 3, 3, 3, 2, 3, 0, 3, 4, 2, 2, 3, 4, 3, 3, 4, 0, 0, 1, 2,\n",
       "       2, 4, 1, 1, 2, 0, 3, 0, 4, 2, 3, 3, 3, 1, 0, 3, 1, 2, 1, 0, 4, 4,\n",
       "       2, 2, 0, 1, 4, 4, 2, 1, 1, 4, 4, 0, 4, 3, 0, 4, 3, 0, 1, 0, 0, 0,\n",
       "       4, 3, 4, 4, 4, 3, 2, 2, 3, 1, 0, 1, 1, 2, 3, 0, 2, 2, 2, 0, 0, 1,\n",
       "       4, 2, 4, 2, 4, 1, 4, 3, 1, 2, 4, 2, 2, 3, 0, 0, 4, 3, 0, 2, 1, 1,\n",
       "       1, 4, 4, 0, 3, 3, 4, 2, 4, 1, 2, 2, 0, 3, 0, 3, 0, 0, 1, 1, 2, 0,\n",
       "       4, 0, 0, 1, 1, 2, 0, 1, 1, 3, 1, 3, 2, 0, 1, 2, 0, 1, 4, 0, 4, 4,\n",
       "       2, 4, 4, 2, 3, 3, 4, 4, 4, 1, 2, 0, 3, 4, 1, 4, 0, 1, 0, 0, 4, 1,\n",
       "       1, 1, 2, 3, 0, 4, 1, 1, 4, 1, 0, 0, 4, 3, 3, 2, 3, 0, 1, 4, 0, 2,\n",
       "       0, 3, 0, 3, 2, 4, 2, 2, 1, 2, 2, 3, 4, 4, 0, 3, 0, 2, 4, 4, 4, 1,\n",
       "       4, 1, 0, 0, 1, 3, 3, 1, 3, 0, 4, 4, 2, 2, 2, 0, 4, 2, 0, 4, 3, 0,\n",
       "       4, 1, 4, 3, 3, 3, 1, 2, 1, 4, 0, 1, 1, 1, 1, 1, 3, 2, 0, 1, 0, 1,\n",
       "       1, 0, 1, 3, 0, 2, 4, 4, 0, 3, 2, 3, 2, 3, 3, 0, 4, 3, 1, 3, 4, 2,\n",
       "       3, 3, 0, 4, 3, 4, 1, 0, 1, 0, 3, 4, 0, 2, 1, 2, 1, 2, 2, 4, 2, 1,\n",
       "       2, 2, 1, 3, 1, 4, 3, 3, 1, 1, 2, 0, 1, 4, 1, 2, 1, 1, 3, 0, 3, 4,\n",
       "       1, 0, 0, 0, 2, 0, 4, 3, 1, 4, 4, 2, 0, 3, 2, 4, 3, 0, 4, 4, 2, 3,\n",
       "       2, 2, 0, 3, 4, 0, 0, 3, 4, 4, 2, 1, 1, 1, 0, 2, 4, 2, 1, 2, 2, 2,\n",
       "       2, 3, 0, 0, 2, 3, 0, 2, 0, 2, 3, 1, 4, 2, 2, 4, 1, 4, 1, 1, 3, 4,\n",
       "       1, 4, 2, 4, 1, 2, 2, 2, 4, 3, 4, 2, 3, 3, 4, 1, 4, 4, 0, 1, 0, 0,\n",
       "       4, 3, 2, 3, 2, 3, 0, 3, 4, 4, 2, 0, 1, 2, 2, 4, 3, 4, 0, 4, 1, 1,\n",
       "       1, 1, 2, 3, 0, 3, 4, 3, 4, 4, 2, 1, 2, 3, 2, 4, 2, 4, 3, 1, 1, 2,\n",
       "       3, 3, 2, 1, 3, 4, 2, 4, 0, 3, 1, 3, 2, 3, 3, 3, 1, 3, 1, 2, 1, 0,\n",
       "       2, 2, 0, 3, 2, 4, 2, 4, 0, 3, 4, 2, 3, 0, 4, 1, 3, 2, 4, 2, 1, 0,\n",
       "       4, 0, 2, 2, 3, 4, 4, 4, 3, 4, 3, 3, 0, 3, 0, 2, 2, 1, 4, 2, 2, 0,\n",
       "       1, 1, 1, 0, 3, 0, 2, 1, 1, 4, 1, 1, 2, 1, 2, 0, 3, 4, 1, 2, 4, 3,\n",
       "       2, 3, 2, 0, 0, 1, 1, 0, 2, 3, 0, 4, 1, 0, 1, 0, 2, 2, 4, 0, 2, 2,\n",
       "       2, 4, 1, 1, 2, 3, 4, 3, 4, 1, 1, 4, 4, 4, 0, 2, 2, 0, 1, 4, 4, 2,\n",
       "       3, 0, 4, 2, 4, 1, 2, 3, 3, 1, 0, 3, 0, 1, 2, 2, 2, 3, 4, 3, 2, 0,\n",
       "       2, 1, 2, 1, 1, 4, 4, 3, 4, 2], dtype=int64)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 89,  38,  27,  19,  12],\n",
       "       [ 42,  78,  31,  20,  19],\n",
       "       [ 23,  56,  66,  45,  17],\n",
       "       [ 15,  31,  45,  73,  41],\n",
       "       [ 10,  17,  17,  38, 131]], dtype=int64)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We obtein to confussion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, array_predict)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b0a4326b88>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cnf_matrix, annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b0ac0491c8>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAD4CAYAAADbyJysAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd1gUx//A8fdwBypVpNlFBcXeQcXeS9RYY6Kx18SYYo0t1sQSk3xNNMaeaOzRRGNLYmJvaCyIHSsWioCAAtfm98eRExAFE0DgNy+ffZ67ndnd+bjH52Zn93aFlBJFURTl1bN61Q1QFEVRzFRCVhRFySFUQlYURckhVEJWFEXJIVRCVhRFySG0Wb0B/3kH89xlHF91rfqqm5AlCtnbvOomZDr7/Fn+EX8l8mpcdjZC/Nd1FKgxIsM5J/70N/95e5lJ9ZAVRVFyiLz5Nasoyv9fIvf2M1VCVhQlb7HSvOoW/GsqISuKkrf892HoV0YlZEVR8hY1ZKEoipJDqB6yoihKDqF6yIqiKDmE6iEriqLkEOoqC0VRlBxCDVkoiqLkEGrIQlEUJYdQPWRFUZQcQiVkRVGUHEKjTuopiqLkDGoMWVEUJYfIxUMWubfliqIoaREi41O6qxJthBCXhRDXhBDj0yhvJIT4WwhhEEJ0Sza/uhDiqBAiSAhxTgjxRkaaniN7yG/UKkqHqoWREoIjHvPprivojE8fAuDhmI8JbcpR0NaamHg903dcJjxOB0DbSu70rVcSgO+P3mZXUBjWGsHszhVxt8/HljP32XrmPgBjW3mx9cx9roY9zvKYdLpEZo4ZikGvw2g04tugOV3fHpKiTkTYA76bP40ncbGYTCbe6P8u1X39Adi2YRX79mzDysqKPsNHUbVWPWKio/hqxliePI6lW59h1K7fBIAvpo2m/4hxOLu4ZWlM4aEPmD9rElGRDxFC0KZjV17v3itFnc1rV7Hv950AGI1G7ty6wbrtf+Hg6MTJ44f57n9zMZlMtH6tMz16DwBg7vSPuRl8Dd/6Dek3dCQAa1ctoXRZb+o1bJqlMQHMnj6JI4cO4OxciO83/PxMeWzMI2bPmMzdkDvY2ORj/OQZlPHyBuD4kUMsmD8bk8lI+05d6d1vEADTJ43jevAV6jdozJB3PwDg+2WLKeNdjoaNm2V5TABTJ0/g4IF9FCrkwqat258p3/nrdlatWAqAra0tEyZPpVx5HwAOHzrI53NmYTSa6NylG/0HmT+7E8eN5urVKzRs3IT33v8IgKWLF+FdrjxNmjXPlriekUk9ZCGEBlgItARCgAAhxDYp5YVk1W4D/YDRqRZ/AvSRUl4VQhQFTgkh9kgpo1+0zRzXQ3a1t6FbzWIMWH2Gt1f9jZUQtPBJmVhGNCnN7qBQ+q76m5VHbzOskScADvm19K9fksFrzjB49Rn61y+JQz4tfp7OXH4QR59Vf9OpWmEAvNzssBIiW5IxgLW1DRNmL+LTRWuZtfBHzp06yrWLgSnq/LJuBX4NmzNr4RpGjJ/JqoVzAbh76zrH9v/GnMXrGTvzf6z6Zi4mo5Gj+3+jYYv2fPLFcnb8tAaAv48dxNOrfJYnYwCNRsOgd0fx3ZqtfPHdan7dsoHbN4JT1On2Vj++WbmRb1ZupN/QkVSuXgsHRyeMRiOLvviM6Z8vZPHqLez/Yze3bwRz49oVABZ9v4mgc6d5HBdLZEQ4Vy6ez5ZkDNDmtdeZt2Dxc8tXr1yKVzkfVq3bysRpn7Jg/mzA/IXz5dyZzPvft/ywcRt7f9vJzevBBF+9DMCqdVs5d+Zv4uJiiYgI52JQYLYlY4AOnTrzzbdLn1terHgxlq1czcYt2xg89B1mTpsCmOOaM2s6Xy9ayk+//MruXTu4HnyNK5fNcW3cso3Tf58iNjaW8PAwzp8/9+qSMWRmD9kXuCalvC6l1AHrgU7JK0gpb0opzwGmVPOvSCmvJr2+B4QB6f5RppuQhRA+QohxQogFQoj/Jb2ukN5y/4XGSpBPa4VGQH5rKyIe61KUl3ax5eQt8xfN37cf0dDLBQA/T2cCbkUTm2AgNtFAwK1o/Eo7YzBJ8llbobF6ugMGNyjFskO3sjKMFIQQ5C9gC4DRYMBgMDz7gRCC+CfmL4gnT+JwdnEF4NSxA9Rt3AprGxvcCxfDo2hxgq8EodFo0OkSMej1CCEwGg3s/nkd7bu+nS0xFXJ1w6u8+aNga2tHSc8yRESEPbf+vj920aR5GwCuXDxP0WIlKFK0ONbW1jRq3pqjh/ah0WrRJSZiMpnQ6/VYWWlYvXwRbw98J1tiAqheszaOjk7PLb95I5hadeoCUMqzDA/u3yXyYQQXgwIpVqIkRYuXwNramuYt23Jo/59otFoSExNSxLRi8TcMGDYiu0ICoFbtOjg5PT+uatVr4phUXqVqNUJDHwBwPvAcxUuWpHiJElhb29C6bTv2/bUXrbWWhKR9ZdDr0WisWLzwa4a/OzJb4nkuK02GJyHEECHEyWRT8sPWYsCdZO9Dkua9FCGEL2ADBKdX94UJWQgxDvO3ggBOAAFJr9elNZ6SGSLidKwLCGHLUF9+eacujxONnLiZspd/NewxTcqZk1Vjbxfs8mlxzK/FzcGGsJhES73w2ETcHGwIuBlFIVsblvauzo8nQmhQthCXQuOeSfRZzWQ0MuHdXrzzZmuq1PDFy6dyivIuvQdz+K/dvNf7NeZN+ZA+w81HQVEPwynk5mGpV8jVnaiIcOo3bcO5U8eYO3kkXXoN5o9ff6JB83bky58/W+MCCL1/l+Arl/CpWCXN8oSEeE4dP4J/kxYAPAwPw9W9sKXc1c2DhxFhlPQsg5tHYUYO7EmjZq24d/c2UkLZcj7ZEkdGeHmX58BffwBwISiQ0Af3CQ8LJSI8DHePpzG5eXgQHh6GZ+myeBQuwqDe3WnaojV379xGIilXPkv7Nf/Jz1s349+gEQDhYaEULlzEUubuUZiw0FDKlClL4SJFeKtHF1q2bsOd27eRUuJToeKraraZsMrwJKVcIqWsnWxaknxNaaz9pR7aLIQoAqwG+kspTenVT28MeSBQSUqpT7WRL4AgYPZzGjEEGAJQpstoCtftmIGmmznk09LQy4XuSwKITTQws6MPrSq68duFcEudhftu8FGLsrSr7MGZkEeExSZiNMm0//ckGCVM22E+vNJYCb7sVplxW4N4r2lpPBzyszsolEPBkRlu479lpdHw6cIfeRwXy1czxnLnZjAlPMtayo/u20OjFq/Rrmsvrl48x7fzpjJ78TqkTOMzIAS2dvaMmf4lAI9jY/h102o+mDyHZf+bxePYWNp1fQvvCln/hOz4J0+YNWk0Q0aOwdbOPs06xw8foGKV6jgk9TxlGp9rkbQHh44ca5k3ddxI3hszifU/LOX6tSvUrF2XNh27ZkEUGder7yAWzJ/NgLe6UsbLG+9yPmg0mjT3k0g6Cho56mn/ZfyH7zJ6wif8sOI7gq9eobZvPTp07vbMsq9KwIlj/LzlJ1b88CNg/htK7Z+4xoybYJn3/ohhTJoyjWVLFnP18iX86tWnS7ce2dLmVI3LrDWFACWSvS8O3Mt4M4QjsAOYJKU8lpFl0huyMAFF05hfhFRjJskl/9Z5mWQMULtUQe49SiA6Xo/RJNl/9SFVijqmqBPxWMeEXy7S/4fTLDl4E4DHOiNhsTrcHfNZ6rk55CMiLmUvuEv1IuwKCqVyUUf0RsmU7RctJwGzi529AxWq1uTcyaMp5u/fsw2/RuYepHeFquj1icTGRFPI1Z3I8FBLvciIMMtwxj+2rl1Gp579ObrvN0p7+TD4w0lsXPVtlsdiMOiZNWkUTVq2w7/x88cND+zdTeMWbSzvXd08iAh7YHkfER5KIdeUQ2xHD/6Ft09FEuLjuXU9mAnT57H3tx0kJMRnfiAvwc7eno8/mcmKtT8xcdpnREdHUaRocdzcPQgLfRpTeGgorqliOrj/T8pXrER8fDw3gq8x7bP57Nm1/ZXH9I8rly8z45PJfLlgIQULOgPg7uHBgwf3LXXCQh/g5u6eYrl9f+6lYqXKxMfHE3ztCnPmf8WO7duIj38Fcb1EDzkdAYC3EKK0EMIG6Alsy1ATzPW3Aj9IKTdltOnptegDYK8QYpcQYknStBvYC7yf0Y28jNDYRCoXdSCf1ty02iULcuthyp3qVEBr6Q2/7VeCHYHmZHX8ZhS+pZxxyKfFIZ8W31LOHL8ZZVnOIZ8W/7KF2BUURj5rK6SUSAk22qw/txkTHcXjuFgAdIkJnD99gqIlSqWo4+JemKAzAQDcvX0DvU6Ho5MzNes25Nj+39DrdIQ9uMuDe3coW66SZbkHd28TFRlBhao1SUxMQAgrhBDodYlkJSklX82eRgnP0nTp+fxx68dxsQSeOUW9Bk9PypXzqcS9kNs8uHcXvV7Pgb17qNugsaXcYNDzy+a1dH2zL4mJCZaDR5k0XvkqxcbGoE9qw68//0S1GrWws7fHp2JlQm7f5t7dEPR6PXt/34V/o6cxGwx6Nq9fw5tv9ycxId7Sy5RJY8uv2v379xj94XvM+GwOpTxLW+ZXqlyFO7ducTckBL1ex55dO2nc5OnJSL1ez9off6BPv4EkJCRYjnSkfEX7KpMSspTSAIwA9gAXgY1SyiAhxHQhREcAIUQdIUQI0B34TggRlLR4D6AR0E8IcSZpqp5e0184ZCGl3C2EKIf5bGMxzH8WIUCAlNKY3sr/jQv3Y/nrSgQr+9TAaJJcCYvjl3P3GeRfiksPYjkUHEmNEgUZ1sgTKSVnQ2KY/8c1AGITDKw6eptlb5vjXnn0NrEJBsu6+9cvyaqj5jH6Ezei6FqjKKv71+TnMw+ebUgmi46K4LvPp2EymZDShF/DFtTwa8jmH76jdLkK1KrbiF6D3mfZgk/ZvXUtCMHQj6YghKB4qbL4NWzBuKFvYKXR0O+dsVgl+3noxu+/pUff4QDUa9KKr6aPYc8v6+n69tAsjelC4Bn+3PMrnmW8GdHffGjad8h7ll5i+9e7A3DkwJ/UrFOP/AUKWJbVaLUM/3A8k0YNx2Qy0ap9J0qV9rKU/7plAy3adCB//gKULlsOJAzv2406dRtg75DyiCmzTZs4htOnAngUHU3X9s3pP+QdjAbz56hT1ze4deM6s6ZOQGOloVTpMoyfPB0ArVbLB2MnMHrkUExGI+06dqZ02acxbdm4njbtO5E/fwHKepdHSknfnp2p698QhyyOCeDjsR9xKiCA6Ogo2jRvzLB33zOfXAa69ejJ0sWLeBQdzWczzfFoNBp+3PATWq2WcRMm8+6wgZiMJjp27krZpMv8ADauX0uHjq9ToEABvMuVRyLp0bkD/g0b4+CY9XE9IxPvhyyl3AnsTDVvSrLXAZiHMlIvtwZY87LbE2mOT2Yi/3kHs3YDr8BXXbN+XPZVKGRv86qbkOns8+fIS+3/s7wal53Nfx8ALvD6kgznnPifh+So31nnzb2qKMr/X7n4p9MqISuKkreomwspiqLkDEIlZEVRlJxBJWRFUZQcQliphKwoipIjqB6yoihKDqESsqIoSg6hErKiKEpOkXvzsUrIiqLkLaqHrCiKkkNYWalf6imKouQIqoesKIqSU+TefKwSsqIoeYvqISuKouQQKiEriqLkEOqn0y8wu1Pl9CvlMvMPXn/VTcgSU1qWe9VNyHR2ee7xCGZxyZ6Ek5fY2Vj/53WoHrKiKEoOoRKyoihKDqESsqIoSg6hErKiKEpOkXvzsUrIiqLkLeqn04qiKDmEGrJQFEXJKXJvPlYJWVGUvEX1kBVFUXKI3JyQc+/ot6IoShqEEBmeMrCuNkKIy0KIa0KI8WmUNxJC/C2EMAghuqUq6yuEuJo09c1I21UPWVGUPCWz7mUhhNAAC4GWQAgQIITYJqW8kKzabaAfMDrVsoWAT4DagAROJS0b9aJtqh6yoih5Sib2kH2Ba1LK61JKHbAe6JS8gpTyppTyHGBKtWxr4HcpZWRSEv4daJPeBlVCVhQlT3mZhCyEGCKEOJlsGpJsVcWAO8nehyTNy4h/tawaslAUJU95mXN6UsolwJLnrSqtRTLajH+zrOohK4qSp2TikEUIUCLZ++LAvQw2418tqxKyoih5ipWVyPCUjgDAWwhRWghhA/QEtmWwGXuAVkIIZyGEM9Aqad6L257BlSuKouQKQmR8ehEppQEYgTmRXgQ2SimDhBDThRAdzdsSdYQQIUB34DshRFDSspHADMxJPQCYnjTvhXLsGLLJaGTGR/1xLuTGyE/mpyj77ee1HPxtG1YaDQ6OzvR/fyIu7kUAOLx3Bzs2rASg/Rv98W/eHr1exzczxxIVEUbTdl1o2t58ueAP33xGk7ZdKFm2fJbHU8QxH+839rS8d7fPx6Yz99l1Mdwyr4C1FSMaeuJqZ4OVFfwaFMb+a+Z92KhsITpX9QBg67lQDgRHorUSjG5WBhdba367HMHvlyMAGFyvBL9fjuBmZHyWxqTTJTLx/UEYdDqMRiP1Gjfnzf7DU9QJOnuKFQvnczP4KqOmfEb9xi0sZX/u3s7mNcsA6NZ7EM3adECv0/HZpA+JCA+jbafutH29BwCLPp9Bm07dKePtk6UxAYSF3mfW1AlEPozASljRoXM3uvV8O0UdKSUL5n/G8SMHyZc/Px9PmUU5n4oA7P71F35Y+R0AffoPpc1rndDpdEwc/R7hYaF06taTzt16AjDv06l06voG5cpXyPK4Zk+fxJFDB3B2LsT3G35+pjw25hGzZ0zmbsgdbGzyMX7yDMp4eQNw/MghFsyfjclkpH2nrvTuNwiA6ZPGcT34CvUbNGbIux8A8P2yxZTxLkfDxs2yPKa0ZKDnm2FSyp3AzlTzpiR7HYB5OCKtZVcAK15mezm2h/zH9g0UKe6ZZlnJMuWZ9MUqpn39I7X8m7Jp5TcAxMU+Yvu65UyYv5yJX6xg+7rlPI6LIejvY5QqW56pX6/hwJ5fALhz4yomkylbkjHA/ZhExm+/zPjtl/n418vojCYCbkenqNPax4270QmM236J6buv8XbtYmisBHY2GrpWK8ykHVeYtOMKXasVxs5GQ7ViDtx4+ISx2y7RvJyL+f/GuQACsjwZA1hb2zD9i+/4cvkGvli2jtMnjnL5wrkUddw8ivDeuKk0ap7yip/YmEds/GEJcxb9wNxvV7PxhyXExcZwOuAoZcpV4KvlG/jt1y0A3Lh2BSlltiRjAI1Gy7vvj2H1xu18u2ItWzet5+b14BR1jh85SMid2/z4005GfzyVL+bMACDm0SNWLfuWxSvW8d3Kdaxa9i2xMY8IOHaYcj4VWbF2C79u3QTAtSuXkCZTtiRjgDavvc68BYufW7565VK8yvmwat1WJk77lAXzZwNgNBr5cu5M5v3vW37YuI29v+3k5vVggq9eBmDVuq2cO/M3cXGxRESEczEo8JUlY8i8HvKrkCMTcmREGOcCjtCwVcc0y32q1iJf/vwAlC1fmaiHYQAE/X2citV9sXdwws7ekYrVfTl/6hgajRa9LhGT0WhZx89rvuP1XkPSXH9Wq1LEgdDYRCIe61PMlxLyW5t3SX5rK+ISjZhMkmrFHAm8F8tjnZHHOiOB92KpVswRowlsNFYpegQ9ahRh05n72RKHEIICBWwBMBoMGI0GRKqTy+6Fi+JZthwi1S0RzwQcpVotPxwcnbB3cKRaLT9OnziCRqtFl5iIMdm+Wrdy0TM976zk4upm6e3a2tlRqnQZwsNDU9Q5dOAvWrfriBCCSlWqERcby8OIcE4cO0xtv3o4Ojnh4OhEbb96HD96GI1WS2KquJZ/9w0Dho7Itriq16yNo6PTc8tv3gimVp26AJTyLMOD+3eJfBjBxaBAipUoSdHiJbC2tqZ5y7Yc2v9nUkwJmEwm9Ho9VlYaViz+hgHDsi+mtGTmL/WyW45MyBuWfkm3/iMy9Iubg79vp0qtegBEPwynkJu7pczZ1Z3oh+FUrOHLo6hIZo0eSJsuvTlz/AClvHwo6OKWZTG8SD1PZ47cePYHO3suhVPMKT/fdq/MvI4+fH8iBAkUsrXm4ROdpV7kEx2FbK05dy+GggW0zGxXju3nw6hVwpEbD58QFZ99D8A0Go18OKgn/Tq3oFotP8pVrJKh5R5GhOHqXtjy3sXNg4cRYVSv7Ud05EPGvdOHzj37cuLwfsqWq0Ah11ezr+7fu8vVyxepWKlqivkRYaG4ezxtv5u7B+FhoUSEh+LunnJ+RHgotX3rEfkwgmH936Tn2wM4fOAvyvtUxDXZ5/VV8/Iuz4G//gDgQlAgoQ/uJ8UUljJWDw/Cw8PwLF0Wj8JFGNS7O01btObundtIZLb1+J8nN/eQ//UYshCiv5RyZWY2BuDsiUM4ODnj6eXDpcBTL6x79K9d3Lp2kTGffQuATOsyP2E+BB0yZjoABoOBrz55nxGT5rFh2VdEhodSr1lbqvs1yuxQ0qSxEtQq4cT6v5+9AqZaMUduRcUz47dreDjYMLGlF5e2X0pzPVKCScLXB2+Z1yvg45ZezPvzOm/XLoarnTUHrkdy6k5M1saj0fDlsvU8jotl9uRR3LpxjVKlvdJdTspn95UQAo1Gy0eTPwXAYNAzbey7TJj1FSsWzici7AFNWr2Gr3/jTI8jLU+ePGHK+A9576Nx2Nnbp2x/Gp81IcRz49JqtUyZORcwxzX6vaF8Ov8bvvlyLmGh92ndriP+jZpmTSAZ1KvvIBbMn82At7pSxssb73I+aDSa58YEMHLU09s7jP/wXUZP+IQfVnxH8NUr1PatR4fO3Z5ZNqvl5hvU/5eWT3teQfJfv2zbsOqlVnrt4jnOnjjIuIGvs2TuZC6dO8nS+Z88U+/CmRPs2LiKEZPmYW1tA4CzizuR4WGWOlERYRQslLJntW/nT9Rr1o7gS4FotNYMHTuTX1+yjf9F9WKO3Ix8wqM0HuPe2KsQJ26Zx5VDY3WExeko6pSfyCd6XGxtLPUK2doQFZ9yuKOVjxsHgiMp52aHwST56sBNOlctTHaxs3egcvVanD5xJEP1Xd08iAh7YHn/MDyUQqmOWHb9vImmrTpwOegcWmtrRk2Zzaakk4BZzWDQM2XcB7Ro3Z5GTVs+U+7mXpiw0KftDw8LxdXN3Tw/LOV8F9eUveCfN6+nTftOBAWewdramk9mfc4PK77LumAyyM7eno8/mcmKtT8xcdpnREdHUaRocdzcPVLGGhqKa6ojloP7/6R8xUrEx8dzI/ga0z6bz55d20lIyPpzGanl5h7yCxOyEOLcc6ZAwON5y0kpl0gpa0spa3d8o99LNahr33eYt2o7c5b/zJCxM/CpWpvBo1Lm/tvBl1m9cA7vTZ6HY8FClvmVavpx4fRxHsfF8Dguhgunj1Oppp+l/HFcDGcDDlG/WTt0iYnmb1Ih0OsTX6qN/4V/aWcOpzFcAfDwsY7KRRwAcMqvpahTPsJiEzl7N4aqRR2ws9FgZ6OhalEHzt592vO1s9FQo7gjB4IjsdFamXs0EqyzuKfwKDqKx3GxACQmJnD21HGKlfTM0LLV69TjzMljxMXGEBcbw5mTx6hep56lPC42hlPHDtK09WskJiRglTTmp9dl/b6SUjJnxhRKlS7DG73SvkmXf8Mm7Nm5DSklQYFnsbO3x8XVDd+6/gQcO0JszKOkk3lH8K3rb1kuNuYRRw7tp3X7jiQmJCCszHHpsiGu9MTGxqDXm7/of/35J6rVqIWdvT0+FSsTcvs29+6GoNfr2fv7rhS9eYNBz+b1a3jz7f4kJsRbes8yaWw5u+XmMeT0hiw8MN8kI3UGEUDGukKZ5Oc1S/D09qG6XyM2rfyahIQnLJ49EYBCbh68N/lz7B2ceK3nAGZ+NACA194ciL3D05MY29et4LUe/RFCULmmH3/t2MzUEb1o3LZztsRgoxFUKeLA0qO3LfNaJF0d8ceVh2w5+4DhDUoxt6MPAlh76h6xieaTQFvOPWBWe/MVIT+de8Bj3dOTQ12rFWbrOfNJp3N3Y2jt48rcTj78kXQZXFaJehjOgtmfYDKZTz76N2lJnXqNWLviW7zKV8TXvzFXLwUxZ/Io4uJiCDh6gPUrF7Ng1WYcHJ3o/vYgxgzrDUCPPoNxSHbCaeMPS+jWexBCCGr41mPXLxv5YEAPWnfM+kPgwLOn+W3Xdsp4eTOwV1cABr/zPqEPzCdLO3V9g7r+jTh25CBvdWlLvvwFGD/ZfJWFo5MTfQYOZWg/82VtfQcNw9HpaVzfL1tMnwFDEUJQp64/Wzevo/+bnenYpUeWxzVt4hhOnwrgUXQ0Xds3p/+QdzAaDJaYbt24zqypE9BYaShVugzjJ5uH+bRaLR+MncDokUMxGY2069iZ0mWfDktt2Wju8efPX4Cy3uWRUtK3Z2fq+jfEwcExy+NKLQfm2QwTaY0PWQqFWA6slFIeSqNsrZTyrfQ2cPBKVEZ/+51rLDx681U3IUtMaVnuVTch0xWys0m/Ui6Um5POi3g4Wv/nyGrN+CvDOefU5KY56n/yhT1kKeXAF5Slm4wVRVGyW27+ssqxv9RTFEX5NzLzl3rZTSVkRVHylJx4si6jVEJWFCVPycX5WCVkRVHyFtVDVhRFySFycT5WCVlRlLxFndRTFEXJIdSQhaIoSg6hErKiKEoOkYvzsUrIiqLkLaqHrCiKkkPk4nysErKiKHmLuspCURQlh7DKxV1klZAVRclTcnE+VglZUZS8RZ3UUxRFySFy8RBy1idkT1fbrN5EtmtZweVVNyFL/Hj27qtuQqZr7On8qpuQJWqVzJtxZQZ1Uk9RFCWHEOTehJy1jyVWFEXJZlYi41N6hBBthBCXhRDXhBDj0yjPJ4TYkFR+XAjhmTTfWgjxvRAiUAhxUQjxcYba/nKhKoqi5GxCiAxP6axHAywE2gIVgTeFEBVTVRsIREkpvYAvgTlJ87sD+aSUVYBawNB/kvWLqISsKEqeIkTGp3T4AteklNellDpgPdApVZ1OwPdJrzcDzYU500vATgihBQoAOiAmvQ2qhKwoSp5iJUSGJyHEECHEyWTTkGSrKgbcSfY+JGkeadWRUhqAR4AL5uT8GLgP3AY+l1JGptd2dVJPUZQ85WWuspBSLgGWPEvCtcQAACAASURBVKc4rRXJDNbxBYxAUcAZOCiE+ENKef1F7VE9ZEVR8pRMHLIIAUoke18cuPe8OknDE05AJPAWsFtKqZdShgGHgdrpbVAlZEVR8pSXGbJIRwDgLYQoLYSwAXoC21LV2Qb0TXrdDfhTSikxD1M0E2Z2QF3gUnobVEMWiqLkKZl1FbKU0iCEGAHsATTACillkBBiOnBSSrkNWA6sFkJcw9wz7pm0+EJgJXA+qUkrpZTn0tumSsiKouQpmXkvCynlTmBnqnlTkr1OwHyJW+rl4tKanx6VkBVFyVNy8S+nVUJWFCVvUfeyUBRFySHU7TcVRVFyiFzcQVYJWVGUvEX1kBVFUXKI3JuOVUJWFCWP0eTiMYscl5DDQh8wZ/pEoh5GIKysaN+pK13e6J2izuEDf7FqyTdYWVmh0WgY/sFYqlSrCcBvO37hx1VLAejVbzCt2ndCp9MxZexIIsJD6dDlDTp1NV+7/cXsaXTo3APv8hWyJbbFH/bGJn8BrKysEBoNfacveqbO7Ytn+XPNIoxGIwXsHXlr0hcAXD8XwN7Vi5AmE1WbtKVuB3MM2xd9RkTIDcpW96NRj4EAHPl5DW4lyuBdq36Wx6R7EsepDV8Tc/8WIKj95vu4lPZJUSfsaiBnty5FmgzY2DnS5L3ZADy4eIozW5YipYnSdVvi08J82ebx1Z8Tc+8WhSvVocprfQC4sGc9BYt6UrRK3SyPCcBkNDJv9CCcXNwYNmluirJje3fyy/eLcCrkCkCj9l2p37KDue1/7mLPJvPNv1p374tfs7bo9TqWfjqe6IfhNGjTmUbtugCwbuEcGrTtTIky5bIlpllTJ3H44H6cCxXix02/PFP+4/cr+G3XrwAYjEZu3bjOzr0HcXQqyLHDB/nq89kYjUY6dO5Kn/6DAZg6cSzBV6/i37Axw977AICVS7+lrHd5GjVpli1xpaaGLDKRRqNh2MhReJevyJPHjxnevye1fOtRqnRZS52atf2o37AJQgiuX7vCjImjWblhGzGPHvHDisUsWrEeIQTD+79BvYZNCTx7inI+Ffn0i0UM72dOyMFXLyNNpmxLxv/oOeFzbB2c0ixLeBzH76sW0H3MZzi6uvP4URQAJpORP77/mh7j5uBQyJUfpozAq2Y9TEYjAP0/XcLaGR+S+OQxel0C94MvU//13mluI7Od3bqUwj41qdf/Y0wGPQZdYopy3ZM4Tm/+lobDpmLr7E5CbDQA0mTk9ObFNBw+A9uCLuz94iOKVvbDZDLH1HLc1/y1YBz6+McYdIlE3b5CxdY9n9l+Vtn36yY8ipciIf5JmuU1GjSjx5CPUsx7HBvDrg0rGPP5coSAuaMGUsXXn+AL5yhRtjzDJn/O3I8G0KhdF0JuXEVKmW3JGKBdh9fp9sZbTJ+S9r3Se/UdQK++AwA4tP8v1v/4A45OBTEajXw+Zxb/W7QUdw8PBvZ+g4aNm2I0mPfV6o1bGT7gbeJiY0lISODC+UD6Dx6ebXGllovzcfr3shBC+Aghmgsh7FPNb5MVDXJxdcO7vPke0LZ2dpT0LE1EeFiKOgVsbS3fggnx8ZbXJ48fpladejg6OeHg6EitOvUIOHYIrVZLYmIixqQEBrBqyTf0HfxuVoTwr108+iflajfA0dUdADsn83PT7gdfpqBHUQq6F0GjtaZC3SZcO3UEjUaDQZ+INJkwGgwIKysO/fQ9Dbr2fdFmMo0+4QnhwefxrNsKACutNTa2KT4m3Pl7P8Wq1sPW2RxTfoeCAETeuoq9axHsXQtjpbWmRI1G3As8jpWVFqNehzSZMCXFdGHXj1Rs2ytbYgKIiggj6ORR6iX1ejPq4unj+FSrg52DI7b2jvhUq8OFv49jpdGg1+ksX6AAO9Yuo/1bgzK76S9Uo1ZtHJ3S7gyk9vuenbRs0w6AC+cDKV68BMWKl8Da2oYWrdtxcN9f5r+rhERMJhN6gx4rjRVLF3/NoOHvZWUY6crEe1lkuxf2kIUQI4F3gYvAciHE+1LKf451PgV2Z2XjHty/y7Url/CpVOWZskP79rL82/8RHRXJrPkLAYgID8PNo7Cljpu7BxHhYTRu1orfd/3KiIG96NGrH0cO/oW3T0Vc3dyzsvnPEAg2zhmPEIJqTdtTvVn7FOWRD0IwGQysmzUKXUI8tVp3pnKDlsRFReBQyM1Sz6GQK/eCL+FSrBSOLu58P3k4Ff1bEBV6FyklHp5e2RLP44gH5LN34uTar3h07yYFS5SleuchaPPlt9SJDbuHNBnY9/XHGBLj8W7UkVK+zYh/9JACzq6WegUKuhB56wqOhUtgW9CNPz7/gFJ1mhIXfh+JxLl42bSakCW2LF9Ap77DSXxO7xjg7NH9BAedxb1oCboMeA9nNw8eRYZT0PXpZ6qgizuPIsOp4d+UgH17mD92CC06v0XgiUOUKFveMuSR0yTEx3PsyCFGjZsIQHh4KB6Fi1jK3dw9uHD+HJ5lyuJRuDD93+pG6/YdCblzGySU98neo87UcmCezbD0hiwGA7WklHFJjx/ZLITwlFL+jxeczEy6yfMQgM+++IZefV++JxD/5AnTPv6Idz4Yi52d/TPlDZo0p0GT5pw7fZKVS75h3tdLMd9k6Zm2oNFqmTjd/GQVg0HP+A+GMWPu13z7v3mEhd6nZdsO1G/Y9KXb+LLemvIlDs6uPH4UxcY543EpWoISPlUt5SajkQc3r/LG+LkY9DrWTBtJ0bIVnhsXQPPe71jm/TR/Mq0GvM/RX34k7PZ1PCvXolrTdlkWj8lkJDokmOpdhuLiWZ4zW5Zwae9mKrd7OlwiTUai7gTT6J2ZGPWJ/PXVGAp5lufZ28pi+Uuq3mWwZdbhpdOp2eNdLv62gUf3buBevgZl6rXOspjOBxzG3qkgJb18uBr4d5p1qtTxp1ajFlhb23Bo98+sXjCLkTMWkMZuAgQajZZ+o6YCYDQYWDjtI4ZOmM2WFV8TFR6Kb9M2VPFtkGUxvaxDB/ZRtVoNHJ3MRzNp7yrzvvpgzNPhjzHvv8PYSVNZtew7rl29TB2/enTq8tK3c/jPcvMYcnpDFpqkm2QgpbwJNAHaCiG+4AUJWUq5REpZW0pZ+98kY4NBz9QJH9G8dXsaNmnxwrpVa9Tm/t07PIqOws3dg/DQB5ay8LBQXFzdUtTf9tMGWrXtyIXzZ9FaWzNpxjx+XLn0pdv4bzgk9QjtnJzxru3P/eDLKcsLuVG6ah1s8hfA1sGJEuWrEnY7GIdCbsRGhlvqxUZGYF/QJcWyV08doXDpcugTEwgPuUmn9yYTdPgP9IkJWRaPbUFXCji54uJZHoBi1fyJDglOUadAQRc8fGqizZeffPZOuJatzKN7Nyjg5Ep8VISlXnz0Qwo4Fkqx7L3AYziX8MagSyTm/m3q9hvP7YC/MOiyLqbrlwI5H3CYTwZ3Y+X8qVw5d4rvv5yeoo6doxPW1jYA1G/ZgTtJ+7GgixvREU+H16Ifhj3TCz64awt+Tdty43IQGq2W/qOnsXvT9+Qkf/y2yzJcAeYeceiD+5b34WGhzxxdHtj3Jz4VK5MQH8/14KvMnPMFu3dsJyE+Ptva/Q+NEBmecpr0EvIDIUT1f94kJefXAFfg2XGETCCl5PNZn1CqVGm6vdknzTp379y29BqvXr6AXm/A0akgtf38OXXiCLExMcTGxHDqxBFq+/lblouNieHY4QO0bNeRhISEfx7hgi7ViaisoEuItxwC6xLiuRl4CtcSninqeNesR8jlQExGI/rEBO4HX8KlaEmKlClP1IO7RIfdx2jQc/HYPrxq1rMsZzQYOLVnK77tu2NITLT0EKQ0jy1nlfyOzhRwdiU2NASAsCtncfQokaJO0cp1ibgehMloxKBLIPLWZRw8SuBc0pu4iHs8fvgAk0HPndMHKFLZ17KcyWjg6oHtlGvWGaMu0fL1L6V5bDmrdHx7GDOWb2Xa0s30HzWVclVr0ffDKSnqPIp8+kUSGHCIwsVLAVChhh8XzwTwJC6GJ3ExXDwTQIUafpa6T+JiOH/yCL5N26BLTEAIKxACg06XZfG8rLjYWE6fCqBhsiskKlSqTMid29y7G4Jer+OPPTtp0PjpEaVBr2fj2tX06tOfhIT4FJ8/vUGf7TFk5lOns1t6QxZ9gBSf/qTnRvURQnyXFQ06f+40f+z+ldJlvRnax3y4M2DYSMKSvqE7dOnBwX1/8Puu7Wi1Wmzy5WPSzLkIIXB0cqJX/6G8O+BNAHoPGJbiJMbqFYvp1W8IQgjq+NVn20/rGdy7K691zvrDqicx0Wz9aipgPtSvWK8pZarW4fTe7QDUaN4Bl2KlKF21DisnDEEIK6o2aYtbidIAtOgzgk3zPkaaTFRp1BrX4p6WdZ/+YxuVG7bEOl9+3EqWQUrJio8HU6aaL/nTGO7JTDW6DOXEmvmYDAbsXDyo/dYHBB/eBUBZ/7Y4Fi5B4Qq1+H3uewghKF23FU5FzAmsetdhHFz8CdJkwtOvhWU+QPDBHZSq0wytTX6cinqChN/mjKBwhdrPnDjMDjvWLqOklw9VfBuwf8dmAk8cwkqjwc7ekV4jzWOtdg6OtOnRl3mjzUMubd/oh52Do2UduzasonX3vgghqFDDl4M7t/DZ+31o0Pr1bIlhysejOX0qgOjoaDq1acagYe9iSPpy69ztDQD2//UHvnX9KVDA1rKcVqvlo3ET+fDdIRhNJl7r2JkyZZ+ep/hp4zradehE/gIF8PIuj5SS3j1ep55/QxySxZ9dcmKizSiR1vhkZroTmZi1G3gFfrsW+qqbkCWuP8y6oYBXpbGn86tuQpaoVTJvxuVip/3P6XTU9ssZzjnzO5TPUek7x12HrCiK8l/k5h6ySsiKouQpOfBcXYaphKwoSp6izcUZWSVkRVHylFycj1VCVhQlb8mJP4nOKJWQFUXJU3JxPlYJWVGUvEVdZaEoipJDqBvUK4qi5BC5OB+rhKwoSt4icvFT9VRCVhQlT8nNPeR0nxiiKIqSm2Tm3d6EEG2EEJeFENeEEOPTKM8nhNiQVH486b7x/5RVFUIcFUIECSEChRD5Uy//TNtfLlRFUZScTSTdVjcjUzrr0QALgbZAReBNIUTFVNUGAlFSSi/gS2BO0rJaYA0wTEpZCfO95NO9F6lKyIqi5Ckaq4xP6fAFrkkpr0spdcB6oFOqOp2Af54wsBloLsyZvhVwTkp5FkBK+VBKaSQdKiEripKnvMxDToUQQ4QQJ5NNQ5KtqhhwJ9n7kKR5pFUn6V7xjwAXoBwghRB7hBB/CyHGZqTt6qSeoih5ysuc1JNSLgGWPKc4rTWlvtfy8+pogQZAHeAJsFcIcUpKufdF7cnyhOxQIO/l/HrFXdKvlAuVcHj+U5Zzq02BefNhAnbavPd3BeDv/d9vvJ+JP50OAZI/k6w4cO85dUKSxo2dgMik+fullBHmNomdQE3ghQlZDVkoipKnWCEyPKUjAPAWQpQWQtgAPYFtqepsA/omve4G/CnNj2HaA1QVQtgmJerGwIX0Npg3v2YVRfl/K7N6yFJKgxBiBObkqgFWSCmDhBDTgZNSym3AcmC1EOIa5p5xz6Rlo4QQX2BO6hLYKaXckd42VUJWFCVP0WbiL0OklDuBnanmTUn2OgFI8ynJUso1mC99yzCVkBVFyVPU7TcVRVFyCHWDekVRlBwiF+djlZAVRclbcvOlYyohK4qSp6ghC0VRlBxCJWRFUZQcIvemY5WQFUXJY3JxB1klZEVR8pb07nOck6mErChKnqKuslAURckh1Ek9RVGUHEINWSiKouQQashCURQlh1A95Ew045OJHD6wH+dChVj3U+p7QcOpgBOM+XAERYuaH23VpHlLBg19B4Cjhw/yxdzPMJmMdOzcjb4DBgMw5eMxBF+7in/Dxrwz8kMAli/5Fi/vcjRu2jxb4tLpEpn4/iAMOh1Go5F6jZvzZv/hKeoEnT3FioXzuRl8lVFTPqN+4xaWsj93b2fzmmUAdOs9iGZtOqDX6fhs0odEhIfRtlN32r7eA4BFn8+gTafulPH2yZbYTEYj80YPwsnFjWGT5qYoO7Z3J798vwinQq4ANGrflfotOwBw/M9d7Nlkfj5k6+598WvWFr1ex9JPxxP9MJwGbTrTqF0XANYtnEODtp0pUaZclsfj4WDD0LpPHxTham/DL+fD2Hv1oWVetaIOvF7ZAyklRgkbztznWoT5iSv1ShWkfUU3AHZcCOforWi0VoJ3/UvibGvNvmuR7AuOBODtWkXZFxzJneiELI8LzPtq+of9KejixgefzE9Rdvn8adYt/ZKQG8EMGzuD2g2aWcoO793B9vUrAejQsz/+zduj1+v4esZYoiLCaNq+C83adwNg1def0bRdF0qVLZ8tMaWWe9NxDkzIr3XsTPeevZg2afxz61SvUYsvvv42xTyj0ci8z2by9eJluHt40K/XGzRs3BSj0fyg1x83/cyQ/r2Ji40lISGBC+cDGThkeFqrzxLW1jZM/+I7ChSwxWDQM+G9gdT086d8xaqWOm4eRXhv3FR+2bA6xbKxMY/Y+MMS5i1egxCC0UN74evfmAvnTlOmXAUmzf6aUUPeou3rPbhx7QpSymxLxgD7ft2ER/FSJMSn/QioGg2a0WPIRynmPY6NYdeGFYz5fDlCwNxRA6ni60/whXOUKFueYZM/Z+5HA2jUrgshN64ipcyWZAwQGqtj+u/BgPma1nmvlef03ZgUdS6FPWbab9cAKOaUj6H1SjJl91VsbTR0qOTOzD+CQUomtfTi7L0YvN3suBUVz4KDt5jcsiz7giMp7pQfIci2ZAzw+7YNFCnhSfyTx8+Uubh5MPCDyezesjbF/LjYR/yydjlTvlqJEIJp7/ejul9DrgSdoZRXeT6Y+gXT3u9Ls/bduH39KlKaXlkyBtDk4h5yjhtuqVGrNo6OTi+93IXzgRQvUZJixUtgbW1Dy9ZtObDvT7RaLYmJiZhMJgx6PVYaK5Ys+poh74zIgtY/nxCCAgVsATAaDBiNBkSq73L3wkXxLFsOYZVyt5wJOEq1Wn44ODph7+BItVp+nD5xBI1Wiy4x0fKlA7Bu5aJnet5ZKSoijKCTR6mX1OvNqIunj+NTrQ52Do7Y2jviU60OF/4+jpVGg16nw5Qsph1rl9H+rUGZ3fQMqeBuT/hjHZFP9CnmJxpMltf5tFb88+zLyh72XAiN44nOyBO9iQuhcVQu7IDRJLHRWGGV7ObpnSq788v5sGyJAyAyIoxzAUdo1KpjmuWuHkUpUdo7RRsBzv99nEo1fLF3cMLO3pFKNXwJPHUMjUaLPjExxb76ec13vN5rSOpVZyshMj7lNOkmZCGErxCiTtLrikKIj4QQ7bK+ac8XeO4MvXp05oN3h3D92lUAwsJC8Shc2FLH3aMw4WFhlC5TFo/CRejTsyvNW7Uh5PZtJJLyPhWzvd1Go5EPB/WkX+cWVKvlR7mKVTK03MOIMFzdn8bm4ubBw4gwqtf2IzryIePe6UPnnn05cXg/ZctVoJCrW1aF8IwtyxfQqe/wF15qdPbofj57vy/L50wiKtz80NFHkeEUdHW31Cno4s6jyHB8qtchJuoh88cOoUXntwg8cYgSZctbhjyyW52STpy4/SjNshrFHJjexpuRDUqxKuAuAAVttUQlS95R8XoK2mq5EBqHY34tE5qXYfflCKoVdeB2VDyPEgzZEgfAuiVf0n3AiJceY41+GE6hZPvK2cWd6IfhVKrhy6PoSGaOGkjbrr05ffwApbx8cHbJvs9fWsRL/MtpXjhkIYT4BGgLaIUQvwN+wD5gvBCihpRyVtY3MaXyFSryy64/sLW14/DB/Yz58D1+2r4bZOqncz/9Bvxo7MeWeaNGvsP4SVNZuXQxV69cxrdufV7vmuYTWDKdRqPhy2XreRwXy+zJo7h14xqlSnulu5xMMzaBRqPlo8mfAmAw6Jk29l0mzPqKFQvnExH2gCatXsPXv3Gmx/GP8wGHsXcqSEkvH64G/p1mnSp1/KnVqAXW1jYc2v0zqxfMYuSMBWntLsAcU79RUwHzkcTCaR8xdMJstqz4mqjwUHybtqGKb4Msiyk5jZWgWlEHtpx7kGb56buxnL4bi7erLZ0qe/Dl/pukNYIpJZgkLDseYl6vgA8aefLN4dv0qFaYQrbWHL0Vzdl7sVkWy5kTh3As6Iynlw+Xzp16qWXT+vwBaDRaho6ZDoDBYOCLKe8zcvI81i/9iofhodRv3pYafo3+c9tfVk7s+WZUej3kboA/0Ah4F3hdSjkdaA288byFhBBDhBAnhRAnVy1fmmmNBbC3t8fW1g4A/4aNMRoMREdF4e5RmNAHT/9wwkIf4OrmnmLZ/X/tpULFSsTHPyE4+BqfzvuSXTu2kRAfn6ltTI+dvQOVq9fi9IkjGarv6uZBRNjT2B6Gh1IoVS9k18+baNqqA5eDzqG1tmbUlNlsSjoJmFWuXwrkfMBhPhncjZXzp3Ll3Cm+/3J6ijp2jk5YW9sAUL9lB+4EXwagoIsb0RFPD9ejH4Y90ws+uGsLfk3bcuNyEBqtlv6jp7E76SRgdqhc2J7bUQnEJhpfWO9qxBPc7Wywt9EQ/USPs621pcy5gDWP4lP2gpt4uXDkZjRlXApgMEm+O3aH9hWytld57cI5zhw/yJgBr7N47mQunTvJks8/ydCyzq7uRCbbV1EPwyiY6vP3146f8G/ejuBLgWisrRk+bia/rl+VmSFkWCY+dTrbpZeQDVJKo5TyCRAspYwBkFLGA6bnLSSlXCKlrC2lrN1v4OBMbC48jAi3fGMHBZ7DJE04FSxIhUqVuXP7FvfuhqDX6/h9zy4aNW76NBC9ng1r19C77wASEhIshysmk0Sv16e5rcz0KDqKx3HmHlBiYgJnTx2nWEnPDC1bvU49zpw8RlxsDHGxMZw5eYzqdepZyuNiYzh17CBNW79GYkICVkIghECvS8yKUCw6vj2MGcu3Mm3pZvqPmkq5qrXo++GUFHUeRUZYXgcGHKJw8VIAVKjhx8UzATyJi+FJXAwXzwRQoYafpe6TuBjOnzyCb9M26BITEMIKhMCg02VpTMn5lnTixO3oNMvc7G0sr0sWzI/GShCnM3I+NI5KHvbYWltha21FJQ97zofGWeraWltRtYgDR29FY6OxMo88S7DWZO3pnG793mH+99uZt+Jnho2dgU/V2gwZPS1Dy1au6UfQ6eM8jovhcVwMQaePU7nm0331OC6GswGHqN+sHbrERKyS9pVen7Wfv+fJzWPI6V1loRNC2CYl5Fr/zBRCOPGChPxfTBo/mr9PniA6OprXWjVlyPARGAzmhNmle0/+/OM3ftq4Ho1WS758+Zg5ez5CCLRaLaPHT2Tk8MGYTCY6dOpMGS9vy3o3bVhH+w6dyF+gAN7lyiORvNWtE/UbNMLB0TErQkkh6mE4C2Z/gslkxGSS+DdpSZ16jVi74lu8ylfE178xVy8FMWfyKOLiYgg4eoD1KxezYNVmHByd6P72IMYM6w1Ajz6DcUh24nPjD0vo1nsQQghq+NZj1y8b+WBAD1p37JblcaVlx9pllPTyoYpvA/bv2EzgiUNYaTTY2TvSa+REAOwcHGnToy/zRpu/sNu+0Q87h6f7YdeGVbTu3hchBBVq+HJw5xY+e78PDVq/ni0x2GgEFT3sWXPqnmVe47LOAOwPjqJWcUfqlSqI0STRGSVLjt0B4InOyK8Xw5jYoiwA2y+E8UT3tIf9WiV3dlw09zaDHsTR1MuFqa292J90GVx227pmCZ7ePtTwa8SNKxf4ZtY4HsfFcubEIX5eu5SZi9Zh7+BEhzcGMOPDAQB06DkQe4enn79t61bQ4Y3+CCGoXNOPP3dsZsqIXjRp2/mVxJSbfzotnjc+BCCEyCelfOZrTgjhChSRUgamt4HoeOPzN5BL3YvKvsuUslPIo7QvW8vNNgWGvuomZIl+NYq96iZkCX9v5/+cTfdeishwzmnu45qjsvcLe8hpJeOk+RFARFpliqIor1JOvHoio3LcD0MURVH+i1w8YqESsqIoeYvqISuKouQQVrk3H+e8n04riqL8F1ZCZHhKjxCijRDishDimhDimRvsCCHyCSE2JJUfF0J4piovKYSIE0KMzlDbMxijoihKriBeYnrheoTQAAsx/1q5IvCmECL1PRcGAlFSSi/gS2BOqvIvgV0ZbbtKyIqi5CmZ2EP2Ba5JKa9LKXXAeqBTqjqdgH9+ProZaC6SbhYihHgduA4EZbjtGa2oKIqSG7xMDzn5bR6SpuS3qisG3En2PiRpHmnVkVIagEeAixDCDhgHZOznkEnUST1FUfKWlzipJ6VcAix5iTWl/tHJ8+pM4//au/M4m+o/juOvj5mxlJoRZlTjF6GyJEr2QbZsY+wmFLIOSousJVkK/Yp+EUr9RJZIZCcxdlmyJ4Vfy8gsMhvNmO37++NeN8Mwo7nj3nv6PHvcR/ec8z33fj4Pd973zNkGphhjLtzM3fU0kJVSluLES6cjgJJXTAcCv19nTISIeAO+wHlsd8bsICKTAT8gQ0SSjTHTbvSGGshKKUtx4llve4FyIlIaOAOEAl2uGrMC6A7swnZ3zE3Gdj+KIEc9ImOAC9mFMWggK6WsxkmJbIxJE5FBwHrAC/jEGHNMRMYC+4wxK4CPgXkichLblnFobt5TA1kpZSnOvFLPGLMGWHPVvNFXPE8GbvgXLowxY3L6fhrISilL0XtZKKWUm/DgPNZAVkpZy83+EVd3ooGslLIUD87jvA/kgj5eef0Wt1yAbwFXl5AnrNhXkQL5sx/kgeq1H+XqEvJE0oFszwzLlgfnsW4hK6UsxoMTWQNZKWUpeoN6pZRyE7oPWSml3IQGslJKuQndZaGUUm5Ct5CVUspNeHAeayArpSzGgxNZA1kpZSlOvEH9LaeBrJSyFM+NYw1kpZTVeHAiayArpSxFT3tTSik3k5JfAgAADrdJREFU4cG7kDWQlVLW4sF5rIGslLIWvUG9Ukq5CQ/OYw1kpZS1eHAek8/VBVxt9KsjaBBUi3YhrbJcboxh4pvjadWsCR3aBnP8+2OOZSuWLyO4eVOCmzdlxfJlAKSkpBDWtxftQlrx+cL5jrFjX3+N48e/z9tmrjBhzKu0aBRE144hWS6f/+kndA9tR/fQdnTtGELdag+TEB8HwO4d2wht25KOrZsx978fOdYZM2ooT3dqy8z3pzrm/fejGWwN35S3zdhZsSeAlJRLjB7cg5EDujCsX2eWzvvwmjHnoiOZMCyMUQO7MSKsCwf37HAsW/H5HF56th1Denfg8P5dACTExTL25T4M7x/Kvp3hjrHvvjGE2D9i8qSPma935Zdv3mLfkpGOeaMHtGTP5yPYvWg4Kz8YyN3FfbNc96tpAzi7dTJL3+ufaf599xRl69whHPlqNPMm9sTH2/YXgcJC67NvyUiWvR/mmFe7yv1MerldnvR2Q3ITDzfjdoEc0qYdM2bNvu7y7du28usvP7Ny7QZGjxnH+LFjAIiPi2PmjGl8tnAx8xctYeaMaSTEx7Nz+zYqVKzEF8tWsHTJYgBO/PADGSaD8uUr3IqWAGgR3IYp02Zdd3nX7s/y6aIv+XTRl4QNeoEqj1bjTl8/0tPT+fekCbzz/kwWLF3BxnVr+N/pk5z88QQA8xYv49CB/VxITORcTAzfHz1CvQYNtadc8PHJz8iJH/DmBwuYMH0+h/fv4uTxI5nGfLXwE2oENWLC9M8YNHw8c6ZPBuDML6fZvWUDk2YuYuj495gzbTIZ6ens2rKBoMYtef3dj1m99DMAvtu9jVJlH6RI0eJ50se8lbsJGTg907wpn35D9c5vUTN0Imu3HWVE3+ZZrjtl7kZ6vTr3mvkTBofw/vzNPBwyltjEJHq0rQVAj7a1eLzTWxz6IYImtcsDMLxPc976cK2Tu8qe3MR/7uamA1lErv1XcqLHqj3Onb5Zf2sDbN70DcGt2yAiVH6kComJCcTERLNzx3Zq1qqDr58fd/r6UrNWHXZs34a3jzfJycmkp6U5XmP6+1MZMOj5vGzjGlUfq3bDvq709fo1NGnWAoDvjx4hMLAk9waWxMcnP42fbMG28M14e3tzKfkSGRkZpKalks8rHx/NfJ/eYc/lZRuZWLEnsB0UKljoNgDS09JIS0u7dsekCEl/XgTgzz8vUKRoMQD2795KzfpN8cmfH/8S9xJwTyCnfjyGl5cXKSmXSEtNRURIT09j3fKFtGz/dJ71seO7U5yP/zPTvMSLyY7ntxUqgDEmy3XD9/xI4sVL18yv//gDfLnxAADzV35LcINHHMt8vL24raAPqWnpdGlVnfXbjxGXmOSMVm6KSM4f7uaGgSwiK656rATaXZ6+RTVmEh0dRUCJEo7pgIASREdFER0dRYlM8wOIjo6iZq06/HHuHF2f6kSPZ3sTvukbKlSshL9/gCvKz1ZyUhK7d27niUZNAIiJiSKgxN2O5cX9A4iJjqLU/WUIKFGCnl060LBJMyJ++xUMPPhQeVeVfl2e2FNGejojB3ZlwFNP8nDV6pR9qFKm5e269WHH5nU8160Vb49+kWfChgAQ+0cMdxX/67N1VzF/Ys/FUPuJZhzev5vJrz1Pu6592LhqKXUbtaBAwYK3tC+AMQOD+WntOEKbV2PcjNU5Xq+o3+3EJyaRnp4BwJmoWO7xt30hT537DVvmvkyxIoXZdfA03VrVYNaSrXlSf3bySc4f7ia7g3qBwPfAbMBg2+tSDXjnRiuJSF+gL8C0D2bRq0/f3Fd6WRbf6CKS9Te9CN7e3kx821ZuamoqYX178Z9pM3h70ltEnj1LcOsQGjRs5Lz6cmn71nAqP1KVO339bDOybMv2SXrhlRGOea8MHsDQV8cwZ/YsTv50gsdr1CKkXcdbUXK2PLGnfF5evDl9PhcvJDJ13FB++/kUJUuVcSzfFb6eeo1b0aJ9V346fpgZb49h4syF1/0c3nZ7YV4ZOwWAi4kJrFoyjxdem8Ts9yZwMTGRFu27UK585VvS25jpKxkzfSVDnm1K/871GD9zTY7Wy+p0ssvtLly9l4Wr9wIwsm9zPlgUzpN1KtK1VXUiImMZ9u6y626NO58bJm0OZbfLohqwHxgFxBtjwoEkY8wWY8yW661kjPnQGFPNGFPNqWEM+AeUICoy0jEdFRVJcX9/AgJKEJlpfhT+xf0zrbt40QJah7Tl0KGD+Pj4MPmdKXw4a4ZT68utjRvWOn61B9vWY1TkWcd0THQUxa7qa2v4Jh6qUInkpCROn/qJ8ZPeZd3qlSQn3fpfF7PiyT3dXvgOyld+lMP7dmWav2X9CmrUawxAufKVSU29RGJCHHcV8+d8TJRj3Plz0Y7dGZctWzCbkNCe7ArfQOmyD9HnxVdZPOfWfw4Xr91Lm0ZVcjz+XOwFfO8ohJeXLTbuDSjC2Zj4TGPuLu7LYxXvY1X4EYb3fpJuwz7hUmoaT1R/0Km134gzd1mISDMROSEiJ0VkeBbLC4jI5/bl34pIKfv8JiKyX0SO2P+fo4MgNwxkY0yGMWYK0BMYJSLTcPGpcg2eaMjKFcsxxnD40EEKF76D4sX9qV2nLrt2bichPp6E+Hh27dxO7Tp1HeslxMezdUs4wSFtSE5OIl++fIgIKSnX7idzlQuJiRzYv5egKw5gla9YiYjffuX3MxGkpqawcf0a6tZ/wrE8LTWVxQvm0fWZniQnJzm2Yoyx7Yd1NU/sKSEulosXEgFIuZTM0QN7uKfkfZnGFPUvwbGDti3CM7/+j9SUFO70LcKjNYPYvWUDqSkpREeeIfL33yjzQEXHepFnfiX2/DnKV36US5eSEbF9DlNv0eewzL/+OoDYsn5lfvw56gajr7V134+0a1wVgK7BNVgVfjjT8tEDWjL2g1UAFCyQH2MgI8NwWyGfXFaec846yUJEvIDpQHOgAvCUiFx9JkAvINYYUxaYAkyyzz8HBBtjHga6A/NyUnuOwtUYEwF0FJGWQEJO1vm7hg15iX179xAXF0uThvUIG/ic7aAK0KnzUwTVq8/2rVto1bwJBQsWYuz4NwHw9fOjb/8BdOncAYB+YQPx9fNzvO6sGdPp0y8MEaF2nSAWLVxA+zbBdOwcmpftOIweMYQD+/cSFxdHSLOG9O4/0NFX2w6dAdiyeSPVa9ahkP2AEoC3tzcvDRvFiwP7kp6RQavWbbm/TFnH8qWLF9IiOISChQpRttyDGGPo1qkNteoEcccdd2pPf0Nc7Dlm/fsNMjIyMCaDGkGNqVojiC/mzqL0A+V5rGY9uvYezOz/vMm6ZQtAhH4vjUZECLyvDDWCGjOsX2fyeXnRY8BQ8nl5OV578acz6NQ9DIBaDZoydewrrP9qEe2f7uf0Pj59qwdBj5WjmF9hTq4bx7iZa2hWtyLl7vMnI8Pw69nzPD9hEQCPVvgXvTvUZcDYBQBs/PgFHigdQOFCBTi5bhz931jAxl3HGfXeV8yb2JPXB7Ti0InfmLP8r98cHnkwEIBDJyJs7798J/uWjCQiMpYJs27d2RZOPFhXHThpjDlte11ZBIRg2417WQgwxv78C2CaiIgx5sAVY44BBUWkgDHmht+8ktf7dZLTstpj6NkuXkrLfpByC6ejLrq6hDxRr/0oV5eQJ5IOTMt1nEYmpOY4c+72zd8P+/Euuw+NMR8CiEgHoJkxprd9+mmghjFm0OXBInLUPibCPn3KPubcFWM6AP2NMY2zq0ev1FNKWcrNJLo9fK+98uf6L3V12N9wjIhUxLYbo2lO6nG7C0OUUio3nHhQLwIoecV0IPD79caIiDfgC5y3TwcCy4BnjDGnclK7BrJSylKceKXeXqCciJQWkfxAKHD19RcrsB20A+gAbDLGGBHxA1YDI4wxO8ghDWSllLU46TQLY0waMAhYDxwHFhtjjonIWBFpbR/2MVBURE4CLwGXT40bBJQFXhORg/aHP9nQg3p/gx7U8xx6UM+zOOOg3rkLaTnOnGKFvd3qKhI9qKeUspR87niTihzSQFZKWYoH57HuQ1ZKKXehW8hKKUvx5C1kDWSllKW4443nc0oDWSllKbqFrJRSbkIDWSml3ITuslBKKTehW8hKKeUmPDiPNZCVUhbjwYmsgayUshRPvnQ6z28udCuJSN/Ld/u3Eiv2ZcWewJp9WbEnd2W1S6ed+yeu3YcV+7JiT2DNvqzYk1uyWiArpZTH0kBWSik3YbVAtup+Liv2ZcWewJp9WbEnt2Spg3pKKeXJrLaFrJRSHksDWSml3IQlAllEmonICRE5KSLDs1/D/YnIJyISLSJHXV2LM4lISRHZLCLHReSYiAx2dU25JSIFRWSPiByy9/SGq2tyJhHxEpEDIrLK1bVYnccHsoh4AdOB5kAF4CkRqeDaqpxiDtDM1UXkgTTgZWNMeaAmMNAC/16XgIbGmEeAKkAzEanp4pqcaTBw3NVF/BN4fCAD1YGTxpjTxpgUYBEQ4uKacs0YsxU47+o6nM0Yc9YY8539eSK2H/R7XVtV7hibC/ZJH/vDEkfLRSQQaAnMdnUt/wRWCOR7gd+umI7Aw3/A/ylEpBRQFfjWtZXknv3X+oNANPC1Mcbje7KbCgwFMlxdyD+BFQI5qzuJWGLrxMpEpDCwFHjBGJPg6npyyxiTboypAgQC1UWkkqtryi0RaQVEG2P2u7qWfworBHIEUPKK6UDgdxfVonJARHywhfF8Y8yXrq7HmYwxcUA41tj/XwdoLSI/Y9sV2FBEPnNtSdZmhUDeC5QTkdIikh8IBVa4uCZ1HSIiwMfAcWPMu66uxxlEpLiI+NmfFwIaAz+4tqrcM8aMMMYEGmNKYfu52mSM6ebisizN4wPZGJMGDALWYztAtNgYc8y1VeWeiCwEdgEPikiEiPRydU1OUgd4GtvW1kH7o4Wri8qlu4HNInIY2wbC18YYPUVM3TS9dFoppdyEx28hK6WUVWggK6WUm9BAVkopN6GBrJRSbkIDWSml3IQGslJKuQkNZKWUchP/B16XWAFI+tqnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#percentage of my data is represented in each quadrant\n",
    "sns.heatmap(cnf_matrix/np.sum(cnf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Experimenting with different regularization strategies</h2>\n",
    "<p>There are many different ways to mitigate overfitting in a neural network, collectively known as <em>regularization</em> techniques. One common regularization technique is called <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\">Dropout</a>. In this regularization method, a set of neurons is randomly selected at each training step to be completely ignored. This is done so that the neurons in our network do not rely strongly on their neighboring neurons and we avoid the creation of <a href=\"http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf\">\"co-adaptations\"</a> that do not generalize well to unseen data. This making the model more robust and less prone to overffiting.</p>\n",
    "<p>You can create dropouts in <code>keras</code> by adding a layer named <code>Dropout(p)</code>, where <code>p</code> is the probability of dropping neurons in the previous layer. For example, the following model would implement dropout by removing roughly 20% percent of the outputs of the embedding layer at each training step:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.5887 - acc: 0.2516 - val_loss: 1.5359 - val_acc: 0.3275\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 1.4312 - acc: 0.3887 - val_loss: 1.3404 - val_acc: 0.4275\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 1.1987 - acc: 0.5056 - val_loss: 1.2788 - val_acc: 0.4550\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 0.9507 - acc: 0.6256 - val_loss: 1.3076 - val_acc: 0.4663\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.7081 - acc: 0.7362 - val_loss: 1.4171 - val_acc: 0.4487\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.4717 - acc: 0.8534 - val_loss: 1.5863 - val_acc: 0.4588\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.2877 - acc: 0.9162 - val_loss: 1.8242 - val_acc: 0.4375\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1718 - acc: 0.9519 - val_loss: 2.0511 - val_acc: 0.4387\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1009 - acc: 0.9797 - val_loss: 2.1791 - val_acc: 0.4400\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0613 - acc: 0.9900 - val_loss: 2.3528 - val_acc: 0.4350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b111b7d188>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(20000, 128, input_length=116))\n",
    "model2.add(Dropout(0.2)) # --------------------------->Dropout layer will affect the output of previous layer.\n",
    "model2.add(Dense(128, activation='relu')) \n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(5, activation='sigmoid'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.fit(train_sequences, labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 4:</h3>\n",
    "<p>Modify the neural network definition above to try and fix the overfitting problem using Dropout. Explain the configuration that you tried and your results. Why do you think your modifications were or were not able to mitigate the overfitting problem?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 160us/sample - loss: 2.2815 - acc: 0.4560\n",
      "Accurancy:  0.456\n"
     ]
    }
   ],
   "source": [
    "y_test = test.Score.to_numpy()\n",
    "loss ,acc = model2.evaluate(test_sequences, y_test)\n",
    "\n",
    "print(\"Accurancy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results vary given the stochastic nature of the algorithm, therefore it was considered to run the example several times and compare the average result. In this case, 5 tests are carried out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45220000000000005"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = [0.455, 0.45, 0.45, 0.449, 0.457]\n",
    "np.mean(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for this case and for the chosen configuration, the use of dropout in the hidden layers raised the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.5962 - acc: 0.2463 - val_loss: 1.5592 - val_acc: 0.3025\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 1.4780 - acc: 0.3537 - val_loss: 1.4079 - val_acc: 0.3963\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 1.2700 - acc: 0.4712 - val_loss: 1.3073 - val_acc: 0.4250\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.0408 - acc: 0.5819 - val_loss: 1.2880 - val_acc: 0.4613\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.8108 - acc: 0.6956 - val_loss: 1.3460 - val_acc: 0.4913\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.5903 - acc: 0.7947 - val_loss: 1.4954 - val_acc: 0.4512\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.3851 - acc: 0.8825 - val_loss: 1.6226 - val_acc: 0.4487\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.2473 - acc: 0.9334 - val_loss: 1.8162 - val_acc: 0.4425\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.1588 - acc: 0.9638 - val_loss: 1.9570 - val_acc: 0.4575\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.0974 - acc: 0.9831 - val_loss: 2.1351 - val_acc: 0.4387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b113618e08>"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We modify the output value to 0.3\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(20000, 128, input_length=116))\n",
    "model3.add(Dropout(0.3)) # --------------------------->Dropout layer will affect the output of previous layer.\n",
    "model3.add(Dense(128, activation='relu')) \n",
    "model3.add(Dense(128, activation='relu'))\n",
    "model3.add(GlobalMaxPooling1D())\n",
    "model3.add(Dense(5, activation='sigmoid'))\n",
    "model3.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model3.fit(train_sequences, labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 155us/sample - loss: 2.1703 - acc: 0.4410\n",
      "Accurancy:  0.441\n"
     ]
    }
   ],
   "source": [
    "y_test = test.Score.to_numpy()\n",
    "loss ,acc = model3.evaluate(test_sequences, y_test)\n",
    "\n",
    "print(\"Accurancy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4444"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2 =[0.451, 0.457, 0.454, 0.419, 0.441 ]\n",
    "np.mean(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this configuration it is observed that for the chosen configuration, the use of dropout = 0.3 in the hidden layers reduced the performance with respect to dropout = 0.2. But the performance is still better than the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.6032 - acc: 0.2375 - val_loss: 1.5811 - val_acc: 0.3300\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 1.5288 - acc: 0.3194 - val_loss: 1.4582 - val_acc: 0.3650\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 1.3506 - acc: 0.4184 - val_loss: 1.3503 - val_acc: 0.4000\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.1660 - acc: 0.5184 - val_loss: 1.2847 - val_acc: 0.4675\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.9695 - acc: 0.6222 - val_loss: 1.2995 - val_acc: 0.4638\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.7757 - acc: 0.7191 - val_loss: 1.4014 - val_acc: 0.4512\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.5825 - acc: 0.8041 - val_loss: 1.5897 - val_acc: 0.4300\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.4145 - acc: 0.8697 - val_loss: 1.7149 - val_acc: 0.4450\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.3042 - acc: 0.9153 - val_loss: 1.8037 - val_acc: 0.4538\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.2055 - acc: 0.9431 - val_loss: 1.9607 - val_acc: 0.4487\n",
      "1000/1000 [==============================] - 0s 179us/sample - loss: 2.0716 - acc: 0.4440\n",
      "Accurancy:  0.444\n"
     ]
    }
   ],
   "source": [
    "#We modify the output value to 0.4\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(20000, 128, input_length=116))\n",
    "model4.add(Dropout(0.4)) # --------------------------->Dropout layer will affect the output of previous layer.\n",
    "model4.add(Dense(128, activation='relu')) \n",
    "model4.add(Dense(128, activation='relu'))\n",
    "model4.add(GlobalMaxPooling1D())\n",
    "model4.add(Dense(5, activation='sigmoid'))\n",
    "model4.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model4.fit(train_sequences, labels, validation_split=0.2, epochs=10)\n",
    "y_test = test.Score.to_numpy()\n",
    "loss ,acc = model4.evaluate(test_sequences, y_test)\n",
    "\n",
    "print(\"Accurancy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44480000000000003"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3 = [0.45, 0.455, 0.438, 0.441, 0.44]\n",
    "np.mean(p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**\n",
    "Taking into account the previous tests, the Dropout values of 0.2, 0.3 and 0.4 were used. Where it was achieved that the highest accuracy was 0.45 using a dropout value = 0.2 ... in this case it may be that using a low dropout value of around 20% constitutes a good starting point. If too low a value is used, the effect of the model's precision is minimal, but if we use a high droput value, we are eliminating many neurons, therefore we have a low learning value in the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 5:</h3>\n",
    "<p>Keras allows you to add <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/l1\">L1</a>, <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/l2\">L2</a>, or <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/l1_l2\">L1 and L2</a> combined regularizers on individual layers by passing in the <code>kernel_regularizer</code>, <code>bias_regularizer</code> or <code>activity_regularizer</code> arguments. In neural networks, these regularizers work by penalizing the loss function in different ways, based on the number of weights or the size of the weights.</p>\n",
    "<p>Try 4-5 different combinations of L1, L2, L1 and L2 regularization in different combinations on different layers. In each example, explain why you tried that configuration and the results. Why do you think your modifications were or were not able to mitigate the overfitting problem?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer1 = tf.keras.layers.Dense(128, kernel_regularizer='l1',activation='relu')\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# This is the first combination l1_l2\n",
    "layer1= layers.Dense(\n",
    "    units=128,\n",
    "    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "    bias_regularizer=regularizers.l2(1e-4),\n",
    "    activity_regularizer=regularizers.l2(1e-5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.6069 - acc: 0.2684 - val_loss: 1.5474 - val_acc: 0.3375\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 1.3978 - acc: 0.4263 - val_loss: 1.3486 - val_acc: 0.4200\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 1.1370 - acc: 0.5519 - val_loss: 1.2775 - val_acc: 0.4737\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 0.8705 - acc: 0.6881 - val_loss: 1.3362 - val_acc: 0.4700\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.6150 - acc: 0.7997 - val_loss: 1.5054 - val_acc: 0.4575\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.4111 - acc: 0.8875 - val_loss: 1.6824 - val_acc: 0.4563\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.2618 - acc: 0.9441 - val_loss: 1.8538 - val_acc: 0.4688\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1722 - acc: 0.9756 - val_loss: 2.0511 - val_acc: 0.4600\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1213 - acc: 0.9903 - val_loss: 2.2241 - val_acc: 0.4613\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0965 - acc: 0.9956 - val_loss: 2.3715 - val_acc: 0.4575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b1698bce48>"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Embedding(20000, 128, input_length=116))\n",
    "model5.add(Dropout(0.2)) # --------------------------->Dropout layer will affect the output of previous layer.\n",
    "model5.add(layer1) \n",
    "model5.add(Dense(128, activation='relu'))\n",
    "model5.add(GlobalMaxPooling1D())\n",
    "model5.add(Dense(5, activation='sigmoid'))\n",
    "model5.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model5.fit(train_sequences, labels, validation_split=0.2, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 185us/sample - loss: 2.3995 - acc: 0.4590\n"
     ]
    }
   ],
   "source": [
    "y_test = test.Score.to_numpy()\n",
    "loss ,acc = model5.evaluate(test_sequences, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "In this first test we tested the l1_l2 configuration in the first layer and the same results were obtained from the reference model. That is, an accuracy of aproximaty 46% was obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.6206 - acc: 0.2444 - val_loss: 1.5863 - val_acc: 0.3262\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 1.4710 - acc: 0.3866 - val_loss: 1.3599 - val_acc: 0.4075\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 4s 1ms/sample - loss: 1.2428 - acc: 0.4822 - val_loss: 1.3313 - val_acc: 0.4475\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.9989 - acc: 0.6103 - val_loss: 1.3488 - val_acc: 0.4588\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.7772 - acc: 0.7212 - val_loss: 1.4796 - val_acc: 0.4412\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.5438 - acc: 0.8356 - val_loss: 1.6901 - val_acc: 0.4387\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.3781 - acc: 0.9006 - val_loss: 1.8696 - val_acc: 0.4313\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.2453 - acc: 0.9500 - val_loss: 2.0702 - val_acc: 0.4400\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1697 - acc: 0.9750 - val_loss: 2.3032 - val_acc: 0.4288\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1225 - acc: 0.9887 - val_loss: 2.4883 - val_acc: 0.4387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b16aa0a488>"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# This is the second test combination l1_l2\n",
    "layer2= layers.Dense(\n",
    "    units=128,\n",
    "    kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n",
    "    bias_regularizer=regularizers.l2(1e-4),\n",
    "    activity_regularizer=regularizers.l2(1e-5)\n",
    ")\n",
    "\n",
    "\n",
    "model6 = Sequential()\n",
    "model6.add(Embedding(20000, 128, input_length=116))\n",
    "model6.add(Dropout(0.2)) # --------------------------->Dropout layer will affect the output of previous layer.\n",
    "model6.add(Dense(128, activation='relu')) \n",
    "model6.add(layer2)\n",
    "model6.add(GlobalMaxPooling1D())\n",
    "model6.add(Dense(5, activation='sigmoid'))\n",
    "model6.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model6.fit(train_sequences, labels, validation_split=0.2, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 193us/sample - loss: 2.6039 - acc: 0.4210\n"
     ]
    }
   ],
   "source": [
    "y_test = test.Score.to_numpy()\n",
    "loss ,acc = model6.evaluate(test_sequences, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "In this test the same configuration is used but on the second layer, in this case it is observed that the precision of the model decreased to 44.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.6003 - acc: 0.2728 - val_loss: 1.5366 - val_acc: 0.3375\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 1.4243 - acc: 0.4147 - val_loss: 1.3794 - val_acc: 0.4000\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 1.1436 - acc: 0.5572 - val_loss: 1.2578 - val_acc: 0.4750\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.8709 - acc: 0.6909 - val_loss: 1.2854 - val_acc: 0.4888\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.5984 - acc: 0.8175 - val_loss: 1.4052 - val_acc: 0.4775\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.3807 - acc: 0.8959 - val_loss: 1.5878 - val_acc: 0.4837\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.2257 - acc: 0.9584 - val_loss: 1.7977 - val_acc: 0.4775\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.1380 - acc: 0.9866 - val_loss: 1.9381 - val_acc: 0.4675\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1012 - acc: 0.9934 - val_loss: 2.0976 - val_acc: 0.4600\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0780 - acc: 0.9972 - val_loss: 2.2173 - val_acc: 0.4600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b177710948>"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#layer1 = tf.keras.layers.Dense(128, kernel_regularizer='l1',bias_regularizer='l2',activation='relu')\n",
    "#layer2 = tf.keras.layers.Dense(128, kernel_regularizer='l2',bias_regularizer='l2',activation='relu')\n",
    "\n",
    "# This is the third test combination l1_l2\n",
    "\n",
    "layer1= layers.Dense(\n",
    "    units=128,\n",
    "    kernel_regularizer=regularizers.l1(1e-5),\n",
    "    bias_regularizer=regularizers.l1(1e-5),\n",
    "    activity_regularizer=regularizers.l1(1e-5)\n",
    ")\n",
    "\n",
    "model7 = Sequential()\n",
    "model7.add(Embedding(20000, 128, input_length=116))\n",
    "model7.add(Dropout(0.2)) # --------------------------->Dropout layer will affect the output of previous layer.\n",
    "model7.add(layer1) \n",
    "model7.add(layer1)\n",
    "model7.add(GlobalMaxPooling1D())\n",
    "model7.add(Dense(5, activation='sigmoid'))\n",
    "model7.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model7.fit(train_sequences, labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 207us/sample - loss: 2.2957 - acc: 0.4690\n"
     ]
    }
   ],
   "source": [
    "y_test = test.Score.to_numpy()\n",
    "loss ,acc = model7.evaluate(test_sequences, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "In this case  we agregate the configuration of l1 regularization to both layers. We can obseve that the accuary of the model improve to aproximately to 47 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 1.6065 - acc: 0.2691 - val_loss: 1.5588 - val_acc: 0.3275\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 1.4146 - acc: 0.4219 - val_loss: 1.3727 - val_acc: 0.4175\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 1.1391 - acc: 0.5519 - val_loss: 1.2908 - val_acc: 0.4762\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 5s 1ms/sample - loss: 0.8661 - acc: 0.6922 - val_loss: 1.3525 - val_acc: 0.4650\n",
      "Epoch 5/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.6041 - acc: 0.8037 - val_loss: 1.4809 - val_acc: 0.4675\n",
      "Epoch 6/10\n",
      "3200/3200 [==============================] - 6s 2ms/sample - loss: 0.3775 - acc: 0.8978 - val_loss: 1.7293 - val_acc: 0.4487\n",
      "Epoch 7/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.2233 - acc: 0.9594 - val_loss: 1.9410 - val_acc: 0.4375\n",
      "Epoch 8/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.1409 - acc: 0.9812 - val_loss: 2.1325 - val_acc: 0.4487\n",
      "Epoch 9/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0961 - acc: 0.9937 - val_loss: 2.2563 - val_acc: 0.4437\n",
      "Epoch 10/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.0778 - acc: 0.9972 - val_loss: 2.3972 - val_acc: 0.4425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b182b13308>"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the third test combination l1_l2\n",
    "\n",
    "layer1_1= layers.Dense(\n",
    "    units=128,\n",
    "    kernel_regularizer=regularizers.l2(1e-5),\n",
    "    bias_regularizer=regularizers.l2(1e-4),\n",
    "    activity_regularizer=regularizers.l2(1e-5)\n",
    ")\n",
    "\n",
    "layer2_2= layers.Dense(\n",
    "    units=128,\n",
    "    kernel_regularizer=regularizers.l1(1e-5),\n",
    "    bias_regularizer=regularizers.l1(1e-4),\n",
    "    activity_regularizer=regularizers.l1(1e-5)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model8 = Sequential()\n",
    "model8.add(Embedding(20000, 128, input_length=116))\n",
    "model8.add(Dropout(0.2)) # --------------------------->Dropout layer will affect the output of previous layer.\n",
    "model8.add(layer1_1) \n",
    "model8.add(layer2_2)\n",
    "model8.add(GlobalMaxPooling1D())\n",
    "model8.add(Dense(5, activation='sigmoid'))\n",
    "model8.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model8.fit(train_sequences, labels, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 205us/sample - loss: 2.3311 - acc: 0.4420\n"
     ]
    }
   ],
   "source": [
    "y_test = test.Score.to_numpy()\n",
    "loss ,acc = model8.evaluate(test_sequences, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "Finally to this case I use the two configurations to the layers. The first layer use the  l2 regularization and to the second layer I use the l1 regularization. We can observed that the accuracy of the model is 44.2%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Regularization through adding more data</h2>\n",
    "<p>Depending on the configurations you tried above, you probably saw that L1 and L2 regularization are pretty limited for this model and this amount of data. A more straightforward way to prevent overfitting is simply by adding more training data. If the network has more (and more varied) examples to learn from, perhaps it will learn more generalizable rules.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 6:</h3>\n",
    "<p>How would you test the hypothesis that adding more data would result in a more generalizable model? Explain any change in results you see from further experimentation.</p>\n",
    "<p><strong>Hint:</strong> Try adding 6000 reviews for each score instead. Compare with the original proposed model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we rwalize the same procedure to take 6000 reviews\n",
    "score_one  = amazon_review[amazon_review[\"Score\"]==1][0:6000]\n",
    "score_one[\"Score\"]=0\n",
    "score_two  = amazon_review[amazon_review[\"Score\"]==2][0:6000]\n",
    "score_two[\"Score\"]=1\n",
    "score_three  = amazon_review[amazon_review[\"Score\"]==3][0:6000]\n",
    "score_three[\"Score\"]=2\n",
    "score_four  = amazon_review[amazon_review[\"Score\"]==4][0:6000]\n",
    "score_four[\"Score\"]=3\n",
    "score_five  = amazon_review[amazon_review[\"Score\"]==5][0:6000]\n",
    "score_five[\"Score\"]=4\n",
    "\n",
    "\n",
    "scores_6000 = pd.concat([score_one,score_two,score_three,score_four,score_five],ignore_index= True)\n",
    "\n",
    "# Now  I divided the dataset in two groups: Train and Test    random_state = 0\n",
    "train_6000, test_6000 = train_test_split(scores_6000, test_size = 0.2, random_state = np.random.seed(1337))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_6000 = train_6000.Text\n",
    "y_train_6000 = train_6000.Score\n",
    "x_test_6000 = test_6000.Text\n",
    "y_test_6000 = test_6000.Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 116)"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer_6000 = Tokenizer(num_words=20000)\n",
    "tokenizer_6000.fit_on_texts(x_train_6000)  #Then, we create the text->indices mapping.\n",
    "\n",
    "# we use the tokenizer to transform the texts in our train data to sequences\n",
    "sequence_train_6000 = tokenizer_6000.texts_to_sequences(x_train_6000)\n",
    "train_sequences_6000 = tf.keras.preprocessing.sequence.pad_sequences(sequence_train_6000,maxlen=116)\n",
    "train_sequences_6000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 116)"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we realize the same procedure with the  test set\n",
    "sequence_test_6000= tokenizer_6000.texts_to_sequences(x_test_6000)\n",
    "# We obtein the array\n",
    "test_sequences_6000 = tf.keras.preprocessing.sequence.pad_sequences(sequence_test_6000,maxlen=116)\n",
    "test_sequences_6000.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the network\n",
    "model_new = Sequential()\n",
    "model_new.add(Embedding(20000, 128, input_length=116))\n",
    "model_new.add(Dropout(0.2)) # --------------------------->Dropout layer will affect the output of previous layer.\n",
    "model_new.add(Dense(128, activation='relu'))\n",
    "model_new.add(Dense(128, activation='relu'))\n",
    "model_new.add(GlobalMaxPooling1D())\n",
    "model_new.add(Dense(5, activation='sigmoid'))\n",
    "model_new.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "labels1 = np.array(np.array(y_train_6000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19200 samples, validate on 4800 samples\n",
      "Epoch 1/10\n",
      "19200/19200 [==============================] - 29s 2ms/sample - loss: 1.3927 - acc: 0.3800 - val_loss: 1.2470 - val_acc: 0.4640\n",
      "Epoch 2/10\n",
      "19200/19200 [==============================] - 30s 2ms/sample - loss: 1.1540 - acc: 0.5128 - val_loss: 1.1866 - val_acc: 0.4940\n",
      "Epoch 3/10\n",
      "19200/19200 [==============================] - 30s 2ms/sample - loss: 1.0188 - acc: 0.5820 - val_loss: 1.2048 - val_acc: 0.5042\n",
      "Epoch 4/10\n",
      "19200/19200 [==============================] - 30s 2ms/sample - loss: 0.8921 - acc: 0.6452 - val_loss: 1.2999 - val_acc: 0.4885\n",
      "Epoch 5/10\n",
      "19200/19200 [==============================] - 30s 2ms/sample - loss: 0.7775 - acc: 0.6993 - val_loss: 1.3431 - val_acc: 0.5002\n",
      "Epoch 6/10\n",
      "19200/19200 [==============================] - 30s 2ms/sample - loss: 0.6599 - acc: 0.7505 - val_loss: 1.4439 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "19200/19200 [==============================] - 30s 2ms/sample - loss: 0.5586 - acc: 0.7958 - val_loss: 1.5900 - val_acc: 0.4948\n",
      "Epoch 8/10\n",
      "19200/19200 [==============================] - 30s 2ms/sample - loss: 0.4716 - acc: 0.8299 - val_loss: 1.7187 - val_acc: 0.4938\n",
      "Epoch 9/10\n",
      "19200/19200 [==============================] - 30s 2ms/sample - loss: 0.3889 - acc: 0.8620 - val_loss: 1.8903 - val_acc: 0.4856\n",
      "Epoch 10/10\n",
      "19200/19200 [==============================] - 30s 2ms/sample - loss: 0.3164 - acc: 0.8899 - val_loss: 2.0498 - val_acc: 0.4900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b18ee2ec48>"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new.fit(train_sequences_6000, labels1, validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 1s 172us/sample - loss: 3.7913 - acc: 0.2173\n",
      "Accurancy:  0.21733333\n"
     ]
    }
   ],
   "source": [
    "y_test_c = y_test_6000.to_numpy()\n",
    "loss ,acc = model.evaluate(test_sequences_6000, y_test_c)\n",
    "\n",
    "print(\"Accurancy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Answer**\n",
    "\n",
    "In this case it is observed that when adding more data, a worse accuracy is obtained. The original model propous had a better accuracity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Regularization through early stopping</h2>\n",
    "<p>We have consistently seen that our neural network overfits at around the third epoch. Hence, another form of regularization is to end training early if validation loss starts increasing. (This is similar to the validation curves we used when constructing classification models.) Although the network will not have found an optimal function in the training data, the looser function that it has found will likely be more generalizable.</p>\n",
    "<p>You can do this manually by inspecting the data as we have done above and modifying the <code>epochs</code> argument in <code>fit()</code>, but Keras also allows you to easily do this automatically via an <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\"><code>EarlyStopping</code> callback</a>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 7:</h3>\n",
    "<p>Experiment with the <code>EarlyStopping</code> callback and explain the results.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In this case we use the model with better accuracity\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/10\n",
      "3200/3200 [==============================] - 7s 2ms/sample - loss: 1.6093 - acc: 0.2744 - val_loss: 1.5532 - val_acc: 0.3762\n",
      "Epoch 2/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.4313 - acc: 0.4238 - val_loss: 1.3523 - val_acc: 0.4212\n",
      "Epoch 3/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 1.1420 - acc: 0.5603 - val_loss: 1.2826 - val_acc: 0.4712\n",
      "Epoch 4/10\n",
      "3200/3200 [==============================] - 5s 2ms/sample - loss: 0.8467 - acc: 0.7003 - val_loss: 1.3383 - val_acc: 0.4450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1= layers.Dense(\n",
    "    units=128,\n",
    "    kernel_regularizer=regularizers.l1(1e-5),\n",
    "    bias_regularizer=regularizers.l1(1e-5),\n",
    "    activity_regularizer=regularizers.l1(1e-5)\n",
    ")\n",
    "\n",
    "model_e = Sequential()\n",
    "model_e.add(Embedding(20000, 128, input_length=116))\n",
    "model_e.add(Dropout(0.2)) # --------------------------->Dropout layer will affect the output of previous layer.\n",
    "model_e.add(layer1) \n",
    "model_e.add(layer1)\n",
    "model_e.add(GlobalMaxPooling1D())\n",
    "model_e.add(Dense(5, activation='sigmoid'))\n",
    "model_e.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model_e.fit(train_sequences, labels, validation_split=0.2, epochs=10, callbacks=[callback])\n",
    "\n",
    "len(history.history['loss'])  # Only 4 epochs are run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "In this case the overfitting starts from the 4th epotch, but the result i simular always the acuracy keep aroun 0.45 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 217us/sample - loss: 1.3738 - acc: 0.4490\n"
     ]
    }
   ],
   "source": [
    "y_test = test.Score.to_numpy()\n",
    "loss ,acc = model_e.evaluate(test_sequences, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Evaluating our model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Unlike in most previous cases, we used <em>three</em> splits of our data instead of two. All of our model tuning has been done on the validation set, and we have not even touched the test set that we split off right at the start.</p>\n",
    "<p>For experiments, it's very important that your model is only run <strong>once</strong> on your test set. As there is so much randomness at play, it's vital to not \"cherry-pick\" the best results, so optimize as much as you want on the validation set, but keep the test set until the end and all official results should be based on the single run of the test set (or whatever configuration was decided <em>before the experiment started</em>).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise 8:</h3>\n",
    "<p>Let's take the model configuration that resulted in the highest validation accuracy and use that one as our final model. Evaluate this configuration on how well it performs on the test set, and furthermore diagnose <em>what kinds of mistakes it makes</em>. Explain whether these mistakes are expected or not, and print some of these poorly classified reviews. Given the mistakes the model made, how would you then go back and try to improve the model or optimize the tuning steps?</p>\n",
    "<p><strong>Hint:</strong> You can use the <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#predict_classes\"><code>predict_classes</code></a> method on your model to get the most probable class directly.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "ans_st"
    ]
   },
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Hopefully, you have seen from this that there is no one-size-fits-all method when creating model architectures or tuning parameters. Often times, copious experimentation is needed, and even then it can be difficult to get significantly better results than a baseline model or even really diagnose what is going wrong under the hood (since neural networks are so \"black-box\"). In many cases, the quantity and quality of the data itself is far more important than the architecture of the network for getting good results.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
